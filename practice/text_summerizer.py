import textwrap
import warnings
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lex_rank import LexRankSummarizer

warnings.filterwarnings("ignore")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) ì¶”ì¶œì  ìš”ì•½ (LexRank)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extractive_summary(text: str, num_sentences: int = 2) -> str:
    parser     = PlaintextParser.from_string(text, Tokenizer("english"))
    summarizer = LexRankSummarizer()
    summary_sents = summarizer(parser.document, num_sentences)
    return " ".join(str(sent) for sent in summary_sents)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) ìƒì„±ì  ìš”ì•½ ëª¨ë¸ ë¡œë”©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_summarizer(model_name="facebook/bart-large-cnn"):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model     = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    return pipeline("summarization", model=model, tokenizer=tokenizer)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) ìƒì„±ì  ìš”ì•½
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def abstractive_summary(text: str,
                        summarizer,
                        min_length: int = 30,
                        max_length: int = 130) -> str:
    out = summarizer(
        text,
        max_length=max_length,
        min_length=min_length,
        do_sample=False
    )
    return out[0]["summary_text"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) í•˜ì´ë¸Œë¦¬ë“œ ìš”ì•½ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def summarize_text(text: str,
                   model_name: str = "facebook/bart-large-cnn") -> str:
    if not text or not isinstance(text, str):
        raise ValueError("ìš”ì•½í•  í…ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    # í† í°(ë‹¨ì–´) ìˆ˜ ê¸°ì¤€
    word_count = len(text.split())
    print(f"ğŸ“ ì…ë ¥ ê¸¸ì´: {word_count} words")

    # 50 ë‹¨ì–´ ì´í•˜: ì›ë¬¸ ë°˜í™˜
    if word_count <= 50:
        print("â„¹ï¸ ì§§ì€ í…ìŠ¤íŠ¸ì´ë¯€ë¡œ ì›ë¬¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return text.strip()

    # 200 ë‹¨ì–´ ì´í•˜: ì¶”ì¶œì  ìš”ì•½
    if word_count <= 200:
        print("ğŸ§©  ì¶”ì¶œì  ìš”ì•½(LexRank)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        return extractive_summary(text, num_sentences=2)

    # ê·¸ ì´ìƒ: ìƒì„±ì  ìš”ì•½
    print("ğŸ¤–  ìƒì„±ì  ìš”ì•½(BART)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
    summarizer = load_summarizer(model_name)
    # ë¨¼ì € ì¶”ì¶œì  ìš”ì•½ìœ¼ë¡œ 2~3ë¬¸ì¥ ë½‘ì•„ì„œ ì••ì¶•
    interim = extractive_summary(text, num_sentences=3)
    return abstractive_summary(interim, summarizer)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5) ì˜ˆì‹œ ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    sample = (
        "autoimmune disease (AI) is a type of autoimmune disease that affects the immune "
        "system. The disease is caused by a lack of immune system function. It affects "
        "the body's ability to fight infections, such as infections of the skin, skin "
        "cells, and skin cells."
    )

    print("\nğŸ“ ì›ë³¸ í…ìŠ¤íŠ¸:")
    print(textwrap.fill(sample, width=80))

    result = summarize_text(sample)
    print("\nâœ… ìš”ì•½ ê²°ê³¼:")
    print(textwrap.fill(result, width=80))