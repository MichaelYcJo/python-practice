import textwrap
import warnings
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM

warnings.filterwarnings("ignore")

#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ìš”ì•½ íŒŒì´í”„ë¼ì¸ ë¡œë”© (ê°•ë ¥í•œ ëª¨ë¸ ê¸°ë³¸)
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_summarizer(model_name="facebook/bart-large-cnn"):
    print(f"ğŸ“¦ ëª¨ë¸ ë¡œë”©: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model     = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    return pipeline("summarization", model=model, tokenizer=tokenizer)

#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì„œ ìš”ì•½
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def chunk_and_summarize(text: str,
                        summarizer,
                        max_chunk_chars: int = 1000,
                        **summ_kwargs) -> str:
    # ë‹¨ìˆœíˆ '\n\n'ë¡œ ë¬¸ë‹¨ ë¶„ë¦¬
    paragraphs = text.split("\n\n")
    chunks = []
    current = ""
    for p in paragraphs:
        # ë¬¸ë‹¨ì„ í•©ì³ë„ max_chunk_chars ì´ë‚´ë©´ ê³„ì† ë¶™ì´ê³ ,
        # ë„˜ì¹˜ë©´ ì§€ê¸ˆê¹Œì§€ ë¬¶ì€ currentë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ë³´ê´€
        if len(current) + len(p) < max_chunk_chars:
            current += p + "\n\n"
        else:
            chunks.append(current)
            current = p + "\n\n"
    if current:
        chunks.append(current)

    summaries = []
    for i, chunk in enumerate(chunks, 1):
        print(f"  â–¶ï¸ ì²­í¬ {i}/{len(chunks)} ìš”ì•½ ì¤‘...")
        out = summarizer(chunk, **summ_kwargs)[0]["summary_text"]
        summaries.append(out)

    # ì²­í¬ë³„ ìš”ì•½ì„ ë‹¤ì‹œ í•œ ë²ˆ í•©ì³ ìš”ì•½
    combined = " ".join(summaries)
    print("  â–¶ï¸ ìµœì¢… í•©ë³¸ ìš”ì•½ ì¤‘...")
    final = summarizer(combined, **summ_kwargs)[0]["summary_text"]
    return final

#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ë©”ì¸ ìš”ì•½ í•¨ìˆ˜
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def summarize_text(text: str,
                   model_name: str = "facebook/bart-large-cnn",
                   min_length: int = 50,
                   max_length: int = 200) -> str:
    if not text or not isinstance(text, str):
        raise ValueError("ìš”ì•½í•  í…ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    print("\nğŸ“ ìš”ì•½ ëŒ€ìƒ í…ìŠ¤íŠ¸ ì¼ë¶€:")
    print(textwrap.shorten(text.strip(), width=300, placeholder="â€¦") + "\n")

    summarizer = load_summarizer(model_name)
    summary = chunk_and_summarize(
        text,
        summarizer,
        max_chunk_chars=800,    # í•œ ë²ˆì— ë„˜ì¹˜ì§€ ì•Šê²Œ
        max_length=max_length,
        min_length=min_length,
        do_sample=False
    )

    return summary

#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ì˜ˆì‹œ ì‹¤í–‰
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    sample_text = """
    ì¸ê³µì§€ëŠ¥(AI)ì€ ì»´í“¨í„° ì‹œìŠ¤í…œì´ ì¸ê°„ê³¼ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ ê¸°ìˆ ì„ ì˜ë¯¸í•œë‹¤.
    AI ê¸°ìˆ ì€ ìŒì„± ì¸ì‹, ìì—°ì–´ ì²˜ë¦¬, ì´ë¯¸ì§€ ë¶„ì„, ì˜ì‚¬ ê²°ì • ë“±ì„ í¬í•¨í•œë‹¤.
    íŠ¹íˆ ìµœê·¼ì—ëŠ” ìƒì„±í˜• AI ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±, ì´ë¯¸ì§€ ìƒì„±, ìŒì•… ì‘ê³¡ ë“± ì°½ì˜ì ì¸ ì˜ì—­ê¹Œì§€ í™œìš© ë²”ìœ„ê°€ í™•ì¥ë˜ê³  ìˆë‹¤.
    ë§ì€ ê¸°ì—…ë“¤ì´ AI ê¸°ìˆ ì„ í™œìš©í•´ ë¹„ì¦ˆë‹ˆìŠ¤ íš¨ìœ¨ì„ ë†’ì´ê³  ìˆìœ¼ë©°, ê´€ë ¨ ì‹œì¥ë„ ë¹ ë¥´ê²Œ ì„±ì¥í•˜ê³  ìˆë‹¤.

    (ì—¬ê¸°ì— í›¨ì”¬ ë” ê¸´ ë³¸ë¬¸ì´ ë“¤ì–´ê°„ë‹¤ê³  ê°€ì •í•˜ì„¸ìš”)
    """

    result = summarize_text(sample_text)
    print("\nâœ… ìµœì¢… ìš”ì•½ ê²°ê³¼:")
    print(textwrap.fill(result, width=80))