import argparse
import csv
import json
import re
import sys
import unicodedata
from collections import Counter
from pathlib import Path
from typing import Iterable, List, Tuple, Set


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸°ë³¸ ë¶ˆìš©ì–´ (ê°€ë³ê²Œ ì‹œì‘, í•„ìš”ì‹œ í™•ì¥)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_STOPWORDS_EN: Set[str] = {
    "the",
    "a",
    "an",
    "and",
    "or",
    "but",
    "if",
    "on",
    "in",
    "at",
    "of",
    "for",
    "to",
    "from",
    "by",
    "as",
    "is",
    "are",
    "was",
    "were",
    "be",
    "been",
    "being",
    "with",
    "that",
    "this",
    "it",
    "its",
    "into",
    "about",
    "than",
    "then",
    "so",
    "such",
    "there",
    "their",
    "they",
    "them",
    "these",
    "those",
    "i",
    "you",
    "he",
    "she",
    "we",
    "us",
    "our",
    "your",
    "his",
    "her",
    "my",
    "me",
}
DEFAULT_STOPWORDS_KO: Set[str] = {
    "ê·¸ë¦¬ê³ ",
    "ê·¸ëŸ¬ë‚˜",
    "í•˜ì§€ë§Œ",
    "ë˜í•œ",
    "ë°",
    "ë“±",
    "ì´",
    "ê°€",
    "ì€",
    "ëŠ”",
    "ì„",
    "ë¥¼",
    "ì˜",
    "ì—",
    "ì—ì„œ",
    "ìœ¼ë¡œ",
    "ì™€",
    "ê³¼",
    "í•˜ë‹¤",
    "í–ˆë‹¤",
    "í•©ë‹ˆë‹¤",
    "í•œë‹¤",
    "ê²ƒ",
    "ìˆ˜",
    "ìˆë‹¤",
    "ì—†ë‹¤",
    "ê°™ë‹¤",
    "ìœ„í•´",
    "ëŒ€í•œ",
    "ì—ì„œì˜",
    "í•˜ì—¬",
    "ì´ëŠ”",
}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í…ìŠ¤íŠ¸ ì •ê·œí™” & í† í°í™”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def normalize_text(text: str, lower: bool = True) -> str:
    # ìœ ë‹ˆì½”ë“œ ì •ê·œí™” (NFKC): ì „ê°/í˜¸í™˜ ë¬¸ì ë“± ì •ë¦¬
    text = unicodedata.normalize("NFKC", text)
    if lower:
        text = text.lower()
    return text


def tokenize(text: str, keep_numbers: bool = True) -> List[str]:
    """
    ê°„ë‹¨ ë©€í‹°ë­ í† í°í™”:
    - ì˜ë¬¸/ìˆ«ì/í•œê¸€ ìŒì ˆì„ 'ë‹¨ì–´'ë¡œ ê°„ì£¼
    - í•„ìš”ì‹œ ìˆ«ì ì œê±°
    """
    # ì˜ë¬¸/ìˆ«ì/í•œê¸€(ê°€-í£)ë§Œ ë‚¨ê¸°ëŠ” í† í°í™”
    tokens = re.findall(r"[A-Za-z0-9ê°€-í£]+", text)
    if not keep_numbers:
        tokens = [t for t in tokens if not t.isdigit()]
    return tokens


def build_stopwords(
    extra: Iterable[str] = None, stopwords_file: Path = None
) -> Set[str]:
    sw: Set[str] = set(DEFAULT_STOPWORDS_EN) | set(DEFAULT_STOPWORDS_KO)
    if stopwords_file and stopwords_file.exists():
        try:
            items = [
                line.strip()
                for line in stopwords_file.read_text(encoding="utf-8").splitlines()
                if line.strip()
            ]
            sw |= set(items)
        except Exception:
            # íŒŒì¼ ì½ê¸° ë¬¸ì œ ì‹œ ì¡°ìš©íˆ ê¸°ë³¸ì…‹ë§Œ ì‚¬ìš©
            pass
    if extra:
        sw |= {s.strip() for s in extra if s.strip()}
    return sw


def apply_stopwords(tokens: List[str], stopwords: Set[str], min_len: int) -> List[str]:
    # ê¸¸ì´ ì œí•œ & ë¶ˆìš©ì–´ ì œê±°
    return [t for t in tokens if len(t) >= min_len and t not in stopwords]


def make_ngrams(tokens: List[str], n: int) -> List[str]:
    if n <= 1:
        return tokens
    return [" ".join(tokens[i : i + n]) for i in range(len(tokens) - n + 1)]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¹´ìš´íŒ… & ì •ë ¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def count_terms(terms: List[str]) -> Counter:
    return Counter(terms)


def sort_counts(
    counter: Counter, by: str = "freq", desc: bool = True
) -> List[Tuple[str, int]]:
    items = list(counter.items())
    if by == "alpha":
        items.sort(key=lambda x: x[0], reverse=desc)
    else:  # freq
        items.sort(key=lambda x: x[1], reverse=desc)
    return items


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì…ì¶œë ¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def read_input_text(
    file: Path = None, encoding: str = "utf-8", text_arg: str = None
) -> str:
    if text_arg:
        return text_arg
    if file:
        return file.read_text(encoding=encoding, errors="ignore")
    # stdin ì§€ì›
    if not sys.stdin.isatty():
        return sys.stdin.read()
    raise ValueError("No input provided. Use --text or --file or pipe from stdin.")


def print_table(rows: List[Tuple[str, int]], total: int = None):
    # ì½˜ì†” ì˜ˆì˜ê²Œ
    if total is not None:
        print(f"ğŸ“„ Total unique: {len(rows)}  |  Total tokens counted: {total}")
    width = max((len(k) for k, _ in rows), default=0)
    for k, v in rows:
        print(f"{k.ljust(width)}  {v}")


def save_output(rows: List[Tuple[str, int]], fmt: str, outpath: Path):
    outpath.parent.mkdir(parents=True, exist_ok=True)
    if fmt == "json":
        data = [{"term": k, "count": v} for k, v in rows]
        outpath.write_text(
            json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8"
        )
    elif fmt == "csv":
        with outpath.open("w", encoding="utf-8", newline="") as f:
            w = csv.writer(f)
            w.writerow(["term", "count"])
            for k, v in rows:
                w.writerow([k, v])
    elif fmt == "txt":
        with outpath.open("w", encoding="utf-8") as f:
            for k, v in rows:
                f.write(f"{k}\t{v}\n")
    else:
        raise ValueError(f"Unsupported output format: {fmt}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    ap = argparse.ArgumentParser(
        description="Word Frequency Counter (multi-lang, n-gram, stopwords, JSON/CSV)"
    )
    src = ap.add_mutually_exclusive_group(required=False)
    src.add_argument("--text", type=str, help="ì§ì ‘ ì…ë ¥ í…ìŠ¤íŠ¸")
    src.add_argument("--file", type=Path, help="í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ")
    ap.add_argument("--encoding", default="utf-8", help="ì…ë ¥ íŒŒì¼ ì¸ì½”ë”© (ê¸°ë³¸ utf-8)")
    ap.add_argument(
        "--lower", action="store_true", help="ì†Œë¬¸ìí™” (ê¸°ë³¸: ì¼œì§)", default=True
    )
    ap.add_argument(
        "--no-lower", dest="lower", action="store_false", help="ì†Œë¬¸ìí™” ë„ê¸°"
    )
    ap.add_argument(
        "--keep-numbers",
        action="store_true",
        help="ìˆ«ì í† í° ìœ ì§€ (ê¸°ë³¸: ì œê±° ì•ˆ í•¨)",
        default=True,
    )
    ap.add_argument(
        "--drop-numbers",
        dest="keep_numbers",
        action="store_false",
        help="ìˆ«ì í† í° ì œê±°",
    )
    ap.add_argument("--min-len", type=int, default=2, help="ìµœì†Œ í† í° ê¸¸ì´ (ê¸°ë³¸ 2)")
    ap.add_argument(
        "--ngram", type=int, default=1, choices=[1, 2, 3], help="n-ê·¸ë¨ í¬ê¸° (1~3)"
    )
    ap.add_argument("--top", type=int, default=0, help="ìƒìœ„ Nê°œë§Œ ì¶œë ¥ (0=ì „ì²´)")
    ap.add_argument(
        "--sort-by", choices=["freq", "alpha"], default="freq", help="ì •ë ¬ ê¸°ì¤€"
    )
    ap.add_argument("--asc", action="store_true", help="ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ (ê¸°ë³¸: ë‚´ë¦¼ì°¨ìˆœ)")
    ap.add_argument("--stopwords-file", type=Path, help="ë¶ˆìš©ì–´ íŒŒì¼(.txt, ì¤„ë‹¨ìœ„)")
    ap.add_argument(
        "--extra-stopwords", type=str, nargs="*", help="ì¶”ê°€ ë¶ˆìš©ì–´ (ê³µë°± êµ¬ë¶„)"
    )
    ap.add_argument("--output", choices=["json", "csv", "txt"], help="íŒŒì¼ë¡œ ì €ì¥ í¬ë§·")
    ap.add_argument("--outpath", type=Path, help="ì €ì¥ ê²½ë¡œ (ë¯¸ì§€ì • ì‹œ ìë™ ìƒì„±)")

    args = ap.parse_args()

    try:
        text = read_input_text(
            file=args.file, encoding=args.encoding, text_arg=args.text
        )
    except Exception as e:
        print(f"âŒ ì…ë ¥ ì˜¤ë¥˜: {e}")
        sys.exit(1)

    # ì •ê·œí™” & í† í°í™”
    text = normalize_text(text, lower=args.lower)
    tokens = tokenize(text, keep_numbers=args.keep_numbers)

    # ë¶ˆìš©ì–´ ì ìš©
    stopwords = build_stopwords(
        extra=args.extra_stopwords, stopwords_file=args.stopwords_file
    )
    tokens = apply_stopwords(tokens, stopwords, min_len=args.min_len)

    # n-ê·¸ë¨
    terms = make_ngrams(tokens, n=args.ngram)

    # ì¹´ìš´íŠ¸ & ì •ë ¬
    counter = count_terms(terms)
    rows = sort_counts(counter, by=args.sort_by, desc=not args.asc)

    # ìƒìœ„ N
    if args.top and args.top > 0:
        rows = rows[: args.top]

    # ì½˜ì†” ì¶œë ¥
    print_table(rows, total=sum(counter.values()))

    # ì €ì¥
    if args.output:
        outpath = args.outpath
        if not outpath:
            # ìë™ íŒŒì¼ëª…
            base = "word_freq"
            suffix = f"_{args.ngram}gram" if args.ngram > 1 else ""
            outpath = Path(f"{base}{suffix}.{args.output}")
        try:
            save_output(rows, fmt=args.output, outpath=outpath)
            print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {outpath}")
        except Exception as e:
            print(f"\nâŒ ì €ì¥ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    main()
