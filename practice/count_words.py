import argparse
import csv
import json
import logging
import re
import statistics
import sys
import time
import unicodedata
from collections import Counter
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Set, Tuple, Union
import warnings

# ì„ íƒì  ì˜ì¡´ì„±ë“¤
try:
    import matplotlib.pyplot as plt
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False

try:
    import yaml
    HAS_YAML = True
except ImportError:
    HAS_YAML = False


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¡œê¹… ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def setup_logging(verbose: bool = False) -> None:
    """ë¡œê¹… ì„¤ì •"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¤ì • íŒŒì¼ ì§€ì›
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_config(config_file: Optional[Path] = None) -> Dict:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ (JSON/YAML ì§€ì›)"""
    if not config_file or not config_file.exists():
        return {}
    
    try:
        content = config_file.read_text(encoding='utf-8')
        if config_file.suffix.lower() in ['.yml', '.yaml'] and HAS_YAML:
            return yaml.safe_load(content) or {}
        else:
            return json.loads(content) or {}
    except (json.JSONDecodeError, yaml.YAMLError if HAS_YAML else Exception) as e:
        logging.warning("ì„¤ì • íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: %s", str(e))
        return {}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í†µê³„ ì •ë³´ ê³„ì‚°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def calculate_stats(counter: Counter) -> Dict[str, Union[int, float]]:
    """ë‹¨ì–´ ë¹ˆë„ í†µê³„ ê³„ì‚°"""
    if not counter:
        return {}
    
    counts = list(counter.values())
    total_words = sum(counts)
    unique_words = len(counts)
    
    stats = {
        'total_words': total_words,
        'unique_words': unique_words,
        'max_frequency': max(counts),
        'min_frequency': min(counts),
        'mean_frequency': statistics.mean(counts),
        'median_frequency': statistics.median(counts),
        'vocabulary_richness': unique_words / total_words if total_words > 0 else 0
    }
    
    return stats


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸°ë³¸ ë¶ˆìš©ì–´ (ê°€ë³ê²Œ ì‹œì‘, í•„ìš”ì‹œ í™•ì¥)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_STOPWORDS_EN: Set[str] = {
    "the",
    "a",
    "an",
    "and",
    "or",
    "but",
    "if",
    "on",
    "in",
    "at",
    "of",
    "for",
    "to",
    "from",
    "by",
    "as",
    "is",
    "are",
    "was",
    "were",
    "be",
    "been",
    "being",
    "with",
    "that",
    "this",
    "it",
    "its",
    "into",
    "about",
    "than",
    "then",
    "so",
    "such",
    "there",
    "their",
    "they",
    "them",
    "these",
    "those",
    "i",
    "you",
    "he",
    "she",
    "we",
    "us",
    "our",
    "your",
    "his",
    "her",
    "my",
    "me",
}
DEFAULT_STOPWORDS_KO: Set[str] = {
    "ê·¸ë¦¬ê³ ",
    "ê·¸ëŸ¬ë‚˜",
    "í•˜ì§€ë§Œ",
    "ë˜í•œ",
    "ë°",
    "ë“±",
    "ì´",
    "ê°€",
    "ì€",
    "ëŠ”",
    "ì„",
    "ë¥¼",
    "ì˜",
    "ì—",
    "ì—ì„œ",
    "ìœ¼ë¡œ",
    "ì™€",
    "ê³¼",
    "í•˜ë‹¤",
    "í–ˆë‹¤",
    "í•©ë‹ˆë‹¤",
    "í•œë‹¤",
    "ê²ƒ",
    "ìˆ˜",
    "ìˆë‹¤",
    "ì—†ë‹¤",
    "ê°™ë‹¤",
    "ìœ„í•´",
    "ëŒ€í•œ",
    "ì—ì„œì˜",
    "í•˜ì—¬",
    "ì´ëŠ”",
}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í…ìŠ¤íŠ¸ ì •ê·œí™” & í† í°í™”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í…ìŠ¤íŠ¸ ì •ê·œí™” & í† í°í™” (ê°œì„ ë¨)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def advanced_normalize_text(text: str, lower: bool = True, 
                           remove_punctuation: bool = True,
                           remove_extra_spaces: bool = True) -> str:
    """ê³ ê¸‰ í…ìŠ¤íŠ¸ ì •ê·œí™”"""
    # ìœ ë‹ˆì½”ë“œ ì •ê·œí™” (NFKC): ì „ê°/í˜¸í™˜ ë¬¸ì ë“± ì •ë¦¬
    text = unicodedata.normalize("NFKC", text)
    
    if remove_punctuation:
        # êµ¬ë‘ì  ì œê±°í•˜ë˜ ë‹¨ì–´ ê²½ê³„ ìœ ì§€
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
    
    if remove_extra_spaces:
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text)
    
    if lower:
        text = text.lower()
    
    return text.strip()


def process_text_chunks(text: str, chunk_size: int = 10000) -> List[str]:
    """ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬"""
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    for i in range(0, len(text), chunk_size):
        chunk = text[i:i + chunk_size]
        # ì²­í¬ ê²½ê³„ì—ì„œ ë‹¨ì–´ê°€ ì˜ë¦¬ì§€ ì•Šë„ë¡ ì¡°ì •
        if i + chunk_size < len(text):
            last_space = chunk.rfind(' ')
            if last_space > chunk_size // 2:  # ì²­í¬ê°€ ë„ˆë¬´ ì‘ì•„ì§€ì§€ ì•Šê²Œ
                chunk = chunk[:last_space]
        chunks.append(chunk)
    
    return chunks


def parallel_tokenize(text: str, keep_numbers: bool = True, 
                     chunk_size: int = 10000) -> List[str]:
    """ë³‘ë ¬ ì²˜ë¦¬ë¡œ í† í°í™” (ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ìš©)"""
    chunks = process_text_chunks(text, chunk_size)
    
    if len(chunks) == 1:
        return tokenize(chunks[0], keep_numbers)
    
    all_tokens = []
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(tokenize, chunk, keep_numbers) for chunk in chunks]
        for future in as_completed(futures):
            all_tokens.extend(future.result())
    
    return all_tokens


def tokenize(text: str, keep_numbers: bool = True) -> List[str]:
    """
    ê°„ë‹¨ ë©€í‹°ë­ í† í°í™”:
    - ì˜ë¬¸/ìˆ«ì/í•œê¸€ ìŒì ˆì„ 'ë‹¨ì–´'ë¡œ ê°„ì£¼
    - í•„ìš”ì‹œ ìˆ«ì ì œê±°
    """
    # ì˜ë¬¸/ìˆ«ì/í•œê¸€(ê°€-í£)ë§Œ ë‚¨ê¸°ëŠ” í† í°í™”
    tokens = re.findall(r"[A-Za-z0-9ê°€-í£]+", text)
    if not keep_numbers:
        tokens = [t for t in tokens if not t.isdigit()]
    return tokens


def build_stopwords(
    extra: Iterable[str] = None, stopwords_file: Path = None
) -> Set[str]:
    sw: Set[str] = set(DEFAULT_STOPWORDS_EN) | set(DEFAULT_STOPWORDS_KO)
    if stopwords_file and stopwords_file.exists():
        try:
            items = [
                line.strip()
                for line in stopwords_file.read_text(encoding="utf-8").splitlines()
                if line.strip()
            ]
            sw |= set(items)
        except (FileNotFoundError, PermissionError, UnicodeDecodeError) as e:
            # íŒŒì¼ ì½ê¸° ë¬¸ì œ ì‹œ ì¡°ìš©íˆ ê¸°ë³¸ì…‹ë§Œ ì‚¬ìš©
            logging.warning("ë¶ˆìš©ì–´ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: %s", str(e))
    if extra:
        sw |= {s.strip() for s in extra if s.strip()}
    return sw


def apply_stopwords(tokens: List[str], stopwords: Set[str], min_len: int) -> List[str]:
    # ê¸¸ì´ ì œí•œ & ë¶ˆìš©ì–´ ì œê±°
    return [t for t in tokens if len(t) >= min_len and t not in stopwords]


def make_ngrams(tokens: List[str], n: int) -> List[str]:
    if n <= 1:
        return tokens
    return [" ".join(tokens[i : i + n]) for i in range(len(tokens) - n + 1)]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¹´ìš´íŒ… & ì •ë ¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def count_terms(terms: List[str]) -> Counter:
    return Counter(terms)


def sort_counts(
    counter: Counter, by: str = "freq", desc: bool = True
) -> List[Tuple[str, int]]:
    items = list(counter.items())
    if by == "alpha":
        items.sort(key=lambda x: x[0], reverse=desc)
    else:  # freq
        items.sort(key=lambda x: x[1], reverse=desc)
    return items


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹œê°í™” (ì„ íƒì )
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def create_frequency_plot(rows: List[Tuple[str, int]], 
                         top_n: int = 20, 
                         save_path: Optional[Path] = None,
                         show: bool = False) -> None:
    """ë‹¨ì–´ ë¹ˆë„ ë§‰ëŒ€ ê·¸ë˜í”„ ìƒì„±"""
    if not HAS_MATPLOTLIB:
        warnings.warn("matplotlibì´ ì—†ì–´ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    # ìƒìœ„ Nê°œë§Œ
    plot_data = rows[:top_n] if len(rows) > top_n else rows
    words, freqs = zip(*plot_data) if plot_data else ([], [])
    
    plt.figure(figsize=(12, 8))
    bars = plt.bar(range(len(words)), freqs)
    plt.xlabel('ë‹¨ì–´')
    plt.ylabel('ë¹ˆë„')
    plt.title(f'ìƒìœ„ {len(words)}ê°œ ë‹¨ì–´ ë¹ˆë„')
    plt.xticks(range(len(words)), words, rotation=45, ha='right')
    
    # ë§‰ëŒ€ ìœ„ì— ìˆ«ì í‘œì‹œ
    for bar, freq in zip(bars, freqs):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(freqs)*0.01,
                str(freq), ha='center', va='bottom')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logging.info("ê·¸ë˜í”„ ì €ì¥ë¨: %s", str(save_path))
    
    if show:
        plt.show()
    else:
        plt.close()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì…ì¶œë ¥ (ê°œì„ ë¨)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def read_input_text(
    file: Path = None, encoding: str = "utf-8", text_arg: str = None
) -> str:
    if text_arg:
        return text_arg
    if file:
        return file.read_text(encoding=encoding, errors="ignore")
    # stdin ì§€ì›
    if not sys.stdin.isatty():
        return sys.stdin.read()
    raise ValueError("No input provided. Use --text or --file or pipe from stdin.")


def print_enhanced_table(rows: List[Tuple[str, int]], 
                        stats: Optional[Dict] = None,
                        show_percentage: bool = False) -> None:
    """í–¥ìƒëœ í…Œì´ë¸” ì¶œë ¥"""
    if not rows:
        print("ğŸ“„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    total = sum(count for _, count in rows)
    
    # í—¤ë” ì¶œë ¥
    if stats:
        print("ï¿½ í†µê³„ ìš”ì•½")
        print(f"   ì´ ë‹¨ì–´ ìˆ˜: {stats.get('total_words', 0):,}")
        print(f"   ê³ ìœ  ë‹¨ì–´ ìˆ˜: {stats.get('unique_words', 0):,}")
        print(f"   ì–´íœ˜ í’ë¶€ë„: {stats.get('vocabulary_richness', 0):.3f}")
        print(f"   í‰ê·  ë¹ˆë„: {stats.get('mean_frequency', 0):.1f}")
        print(f"   ì¤‘ì•™ê°’ ë¹ˆë„: {stats.get('median_frequency', 0):.1f}")
        print()
    
    print("ğŸ“„ ë‹¨ì–´ ë¹ˆë„ ìˆœìœ„")
    print("=" * 50)
    
    # ì»¬ëŸ¼ í­ ê³„ì‚°
    word_width = max((len(word) for word, _ in rows), default=10)
    word_width = max(word_width, 10)  # ìµœì†Œ 10ì
    
    # í—¤ë”
    header = f"{'ìˆœìœ„':<4} {'ë‹¨ì–´':<{word_width}} {'ë¹ˆë„':<8}"
    if show_percentage:
        header += " {'ë¹„ìœ¨':<8}"
    print(header)
    print("-" * len(header))
    
    # ë°ì´í„° í–‰
    for i, (word, count) in enumerate(rows, 1):
        line = f"{i:<4} {word:<{word_width}} {count:<8}"
        if show_percentage:
            percentage = (count / total * 100) if total > 0 else 0
            line += f" {percentage:<7.2f}%"
        print(line)
    
    print("=" * 50)


def save_enhanced_output(rows: List[Tuple[str, int]], 
                        fmt: str, 
                        outpath: Path,
                        stats: Optional[Dict] = None,
                        metadata: Optional[Dict] = None) -> None:
    """í–¥ìƒëœ íŒŒì¼ ì €ì¥"""
    outpath.parent.mkdir(parents=True, exist_ok=True)
    
    if fmt == "json":
        data = {
            "metadata": {
                "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                "total_entries": len(rows),
                **(metadata or {})
            },
            "statistics": stats or {},
            "results": [{"rank": i+1, "term": word, "count": count} 
                       for i, (word, count) in enumerate(rows)]
        }
        outpath.write_text(
            json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8"
        )
    elif fmt == "csv":
        with outpath.open("w", encoding="utf-8", newline="") as f:
            w = csv.writer(f)
            # í—¤ë”ì— ë©”íƒ€ë°ì´í„° í¬í•¨
            if metadata:
                w.writerow([f"# Generated at: {time.strftime('%Y-%m-%d %H:%M:%S')}"])
                for key, value in metadata.items():
                    w.writerow([f"# {key}: {value}"])
                w.writerow([])
            
            w.writerow(["rank", "term", "count"])
            for i, (word, count) in enumerate(rows, 1):
                w.writerow([i, word, count])
    elif fmt == "txt":
        with outpath.open("w", encoding="utf-8") as f:
            if metadata:
                f.write(f"# Generated at: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                for key, value in metadata.items():
                    f.write(f"# {key}: {value}\n")
                f.write("\n")
            
            if stats:
                f.write("# Statistics\n")
                for key, value in stats.items():
                    f.write(f"# {key}: {value}\n")
                f.write("\n")
            
            f.write("rank\tterm\tcount\n")
            for i, (word, count) in enumerate(rows, 1):
                f.write(f"{i}\t{word}\t{count}\n")
    else:
        raise ValueError(f"Unsupported output format: {fmt}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    ap = argparse.ArgumentParser(
        description="Enhanced Word Frequency Counter (multi-lang, n-gram, stopwords, visualization)"
    )
    src = ap.add_mutually_exclusive_group(required=False)
    src.add_argument("--text", type=str, help="ì§ì ‘ ì…ë ¥ í…ìŠ¤íŠ¸")
    src.add_argument("--file", type=Path, help="í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ")
    ap.add_argument("--encoding", default="utf-8", help="ì…ë ¥ íŒŒì¼ ì¸ì½”ë”© (ê¸°ë³¸ utf-8)")
    ap.add_argument("--config", type=Path, help="ì„¤ì • íŒŒì¼ ê²½ë¡œ (JSON/YAML)")
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    ap.add_argument(
        "--lower", action="store_true", help="ì†Œë¬¸ìí™” (ê¸°ë³¸: ì¼œì§)", default=True
    )
    ap.add_argument(
        "--no-lower", dest="lower", action="store_false", help="ì†Œë¬¸ìí™” ë„ê¸°"
    )
    ap.add_argument(
        "--advanced-normalize", action="store_true", 
        help="ê³ ê¸‰ í…ìŠ¤íŠ¸ ì •ê·œí™” ì‚¬ìš©", default=False
    )
    ap.add_argument(
        "--keep-numbers",
        action="store_true",
        help="ìˆ«ì í† í° ìœ ì§€ (ê¸°ë³¸: ì œê±° ì•ˆ í•¨)",
        default=True,
    )
    ap.add_argument(
        "--drop-numbers",
        dest="keep_numbers",
        action="store_false",
        help="ìˆ«ì í† í° ì œê±°",
    )
    ap.add_argument("--min-len", type=int, default=2, help="ìµœì†Œ í† í° ê¸¸ì´ (ê¸°ë³¸ 2)")
    ap.add_argument(
        "--ngram", type=int, default=1, choices=[1, 2, 3], help="n-ê·¸ë¨ í¬ê¸° (1~3)"
    )
    
    # ì¶œë ¥ ì˜µì…˜
    ap.add_argument("--top", type=int, default=0, help="ìƒìœ„ Nê°œë§Œ ì¶œë ¥ (0=ì „ì²´)")
    ap.add_argument(
        "--sort-by", choices=["freq", "alpha"], default="freq", help="ì •ë ¬ ê¸°ì¤€"
    )
    ap.add_argument("--asc", action="store_true", help="ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ (ê¸°ë³¸: ë‚´ë¦¼ì°¨ìˆœ)")
    ap.add_argument("--show-stats", action="store_true", help="í†µê³„ ì •ë³´ í‘œì‹œ")
    ap.add_argument("--show-percentage", action="store_true", help="ë¹„ìœ¨ í‘œì‹œ")
    
    # ë¶ˆìš©ì–´
    ap.add_argument("--stopwords-file", type=Path, help="ë¶ˆìš©ì–´ íŒŒì¼(.txt, ì¤„ë‹¨ìœ„)")
    ap.add_argument(
        "--extra-stopwords", type=str, nargs="*", help="ì¶”ê°€ ë¶ˆìš©ì–´ (ê³µë°± êµ¬ë¶„)"
    )
    
    # ì €ì¥ ì˜µì…˜
    ap.add_argument("--output", choices=["json", "csv", "txt"], help="íŒŒì¼ë¡œ ì €ì¥ í¬ë§·")
    ap.add_argument("--outpath", type=Path, help="ì €ì¥ ê²½ë¡œ (ë¯¸ì§€ì • ì‹œ ìë™ ìƒì„±)")
    
    # ì‹œê°í™”
    if HAS_MATPLOTLIB:
        ap.add_argument("--plot", action="store_true", help="ë§‰ëŒ€ ê·¸ë˜í”„ ìƒì„±")
        ap.add_argument("--plot-path", type=Path, help="ê·¸ë˜í”„ ì €ì¥ ê²½ë¡œ")
        ap.add_argument("--show-plot", action="store_true", help="ê·¸ë˜í”„ í™”ë©´ì— í‘œì‹œ")
    
    # ê¸°íƒ€
    ap.add_argument("--verbose", "-v", action="store_true", help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥")
    ap.add_argument("--parallel", action="store_true", help="ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš© (ëŒ€ìš©ëŸ‰ íŒŒì¼)")

    args = ap.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    setup_logging(args.verbose)
    
    # ì„¤ì • íŒŒì¼ ë¡œë“œ
    config = load_config(args.config)
    logging.info("ì„¤ì • ë¡œë“œ ì™„ë£Œ")

    try:
        start_time = time.time()
        text = read_input_text(
            file=args.file, encoding=args.encoding, text_arg=args.text
        )
        logging.info("í…ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ: %d ë¬¸ì", len(text))
    except (FileNotFoundError, PermissionError, UnicodeDecodeError, IOError) as e:
        print(f"âŒ ì…ë ¥ ì˜¤ë¥˜: {e}")
        sys.exit(1)

    # í…ìŠ¤íŠ¸ ì •ê·œí™”
    if args.advanced_normalize:
        text = advanced_normalize_text(text, lower=args.lower)
        logging.info("ê³ ê¸‰ í…ìŠ¤íŠ¸ ì •ê·œí™” ì™„ë£Œ")
    else:
        # ê¸°ë³¸ ì •ê·œí™”
        text = unicodedata.normalize("NFKC", text)
        if args.lower:
            text = text.lower()
        logging.info("ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ê·œí™” ì™„ë£Œ")
    
    # í† í°í™” (ë³‘ë ¬ ì²˜ë¦¬ ì˜µì…˜)
    if args.parallel and len(text) > 50000:
        tokens = parallel_tokenize(text, keep_numbers=args.keep_numbers)
        logging.info("ë³‘ë ¬ í† í°í™” ì™„ë£Œ: %d í† í°", len(tokens))
    else:
        tokens = tokenize(text, keep_numbers=args.keep_numbers)
        logging.info("í† í°í™” ì™„ë£Œ: %d í† í°", len(tokens))

    # ë¶ˆìš©ì–´ ì ìš©
    stopwords = build_stopwords(
        extra=args.extra_stopwords, stopwords_file=args.stopwords_file
    )
    tokens = apply_stopwords(tokens, stopwords, min_len=args.min_len)
    logging.info("ë¶ˆìš©ì–´ í•„í„°ë§ ì™„ë£Œ: %d í† í°", len(tokens))

    # n-ê·¸ë¨
    terms = make_ngrams(tokens, n=args.ngram)
    logging.info("n-ê·¸ë¨ ìƒì„± ì™„ë£Œ: %d terms", len(terms))

    # ì¹´ìš´íŠ¸ & ì •ë ¬
    counter = count_terms(terms)
    rows = sort_counts(counter, by=args.sort_by, desc=not args.asc)
    
    # í†µê³„ ê³„ì‚°
    stats = calculate_stats(counter) if args.show_stats else None
    
    processing_time = time.time() - start_time
    logging.info("ì²˜ë¦¬ ì™„ë£Œ: %.2fì´ˆ", processing_time)

    # ìƒìœ„ N
    if args.top and args.top > 0:
        rows = rows[: args.top]

    # ì¶œë ¥
    print_enhanced_table(rows, stats, args.show_percentage)

    # ì‹œê°í™”
    if HAS_MATPLOTLIB and args.plot:
        create_frequency_plot(
            rows, 
            top_n=min(20, len(rows)),
            save_path=args.plot_path,
            show=args.show_plot
        )

    # ì €ì¥
    if args.output:
        outpath = args.outpath
        if not outpath:
            # ìë™ íŒŒì¼ëª…
            base = "word_freq"
            suffix = f"_{args.ngram}gram" if args.ngram > 1 else ""
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            outpath = Path(f"{base}{suffix}_{timestamp}.{args.output}")
        
        try:
            metadata = {
                "processing_time": f"{processing_time:.2f}s",
                "input_length": len(text),
                "ngram_size": args.ngram,
                "min_length": args.min_len,
                "total_terms": len(terms)
            }
            save_enhanced_output(rows, fmt=args.output, outpath=outpath, 
                               stats=stats, metadata=metadata)
            print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {outpath}")
        except (IOError, OSError, PermissionError) as e:
            print(f"\nâŒ ì €ì¥ ì‹¤íŒ¨: {e}")
            logging.error("ì €ì¥ ì‹¤íŒ¨: %s", str(e), exc_info=True)


if __name__ == "__main__":
    main()
