import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin


def scrape_website(url, max_links=10, keyword_filter=None):
    """ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì œëª©ê³¼ ëª¨ë“  ë§í¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì›¹ ìŠ¤í¬ë˜í¼"""
    try:
        response = requests.get(url)
        response.raise_for_status()  # ìš”ì²­ì´ ì‹¤íŒ¨í•˜ë©´ ì˜ˆì™¸ ë°œìƒ

        # HTML íŒŒì‹±
        soup = BeautifulSoup(response.text, "html.parser")

        # í˜ì´ì§€ ì œëª© ê°€ì ¸ì˜¤ê¸°
        title = soup.title.string if soup.title else "ì œëª© ì—†ìŒ"
        print(f"\nğŸ“Œ í˜ì´ì§€ ì œëª©: {title}")

        # ëª¨ë“  ë§í¬(a íƒœê·¸) ì¶”ì¶œ & ìƒëŒ€ ê²½ë¡œ ë³€í™˜
        links = [urljoin(url, a["href"]) for a in soup.find_all("a", href=True)]

        # í‚¤ì›Œë“œ í•„í„°ë§ ì ìš©
        if keyword_filter:
            links = [link for link in links if keyword_filter.lower() in link.lower()]

        # ë§í¬ ì¶œë ¥ ê°œìˆ˜ ì œí•œ
        total_links = len(links)
        links = links[:max_links]  # ìµœëŒ€ ê°œìˆ˜ ì œí•œ

        if links:
            print(f"\nğŸ”— í˜ì´ì§€ì˜ ë§í¬ ëª©ë¡ (ìµœëŒ€ {max_links}ê°œ):")
            for link in links:
                print(f"- {link}")

            # ë§í¬ ì €ì¥ ê¸°ëŠ¥ ì¶”ê°€
            with open("scraped_links.txt", "w", encoding="utf-8") as file:
                file.write("\n".join(links))
            print("\nğŸ’¾ ë§í¬ ëª©ë¡ì´ 'scraped_links.txt' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

        else:
            print("âŒ ì¡°ê±´ì— ë§ëŠ” ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ ìš”ì²­ ì‹¤íŒ¨: {e}")


# ì‚¬ìš©ì ì…ë ¥
url = input("ğŸŒ í¬ë¡¤ë§í•  ì›¹ì‚¬ì´íŠ¸ URLì„ ì…ë ¥í•˜ì„¸ìš”: ")
max_links = int(input("ğŸ“Œ ì¶œë ¥í•  ìµœëŒ€ ë§í¬ ê°œìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 10): ") or 10)
keyword_filter = (
    input(
        "ğŸ” íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë§í¬ë§Œ ì°¾ì„ê¹Œìš”? (ì˜ˆ: news, blog) [ì—”í„° ì‹œ ëª¨ë“  ë§í¬ ì¶œë ¥]: "
    ).strip()
    or None
)

# ì‹¤í–‰
scrape_website(url, max_links, keyword_filter)
