import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

SAVE_PATH = "website_images"

# ì €ì¥ í´ë” ìƒì„±
if not os.path.exists(SAVE_PATH):
    os.makedirs(SAVE_PATH)


def get_all_images(url):
    """ì›¹ì‚¬ì´íŠ¸ì—ì„œ ëª¨ë“  ì´ë¯¸ì§€ URL ê°€ì ¸ì˜¤ê¸°"""
    response = requests.get(url)

    if response.status_code != 200:
        print("âŒ ì›¹ì‚¬ì´íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    image_urls = []

    for img_tag in soup.find_all("img"):
        img_url = img_tag.get("src")
        if img_url:
            full_url = urljoin(url, img_url)  # ìƒëŒ€ê²½ë¡œ ì²˜ë¦¬
            image_urls.append(full_url)

    return list(set(image_urls))  # ì¤‘ë³µ ì œê±°


def download_image(image_url):
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥"""
    filename = os.path.basename(urlparse(image_url).path)

    if not filename:  # íŒŒì¼ëª…ì´ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
        return

    save_path = os.path.join(SAVE_PATH, filename)

    if os.path.exists(save_path):
        print(f"âš ï¸ ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ì´ë¯¸ì§€: {filename}")
        return

    response = requests.get(image_url, stream=True)

    if response.status_code == 200:
        with open(save_path, "wb") as file:
            for chunk in response.iter_content(1024):
                file.write(chunk)

        print(f"âœ… ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filename}")
    else:
        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {filename}")


def main():
    """ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
    website_url = input("ğŸŒ ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ì›¹ì‚¬ì´íŠ¸ URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    image_urls = get_all_images(website_url)

    if not image_urls:
        print("âš ï¸ ë‹¤ìš´ë¡œë“œí•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(
        f"\nğŸ¨ ì´ {len(image_urls)}ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\n"
    )

    for img_url in image_urls:
        download_image(img_url)


# ì‹¤í–‰
main()
