"""
Enhanced PDF OCR Processing System
ê°œì„ ëœ PDF OCR ì²˜ë¦¬ ì‹œìŠ¤í…œ - í´ë˜ìŠ¤ ê¸°ë°˜ ì•„í‚¤í…ì²˜
"""

import asyncio
import hashlib
import json
import logging
import time
import warnings
from abc import ABC, abstractmethod
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from contextlib import contextmanager
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any, Protocol, Callable
from io import BytesIO
from functools import wraps

import pytesseract
from PIL import Image, ImageOps, ImageFilter, ImageEnhance
from pdf2image import convert_from_path
from pypdf import PdfReader, PdfWriter
from tqdm import tqdm

try:
    from langdetect import detect as lang_detect
    LANG_DETECT_AVAILABLE = True
except ImportError:
    lang_detect = None
    LANG_DETECT_AVAILABLE = False

try:
    import cv2
    import numpy as np
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False

# ê²½ê³  í•„í„°ë§
warnings.filterwarnings('ignore', category=UserWarning)


class ProcessingStatus(Enum):
    """ì²˜ë¦¬ ìƒíƒœ ì—´ê±°í˜•"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


@dataclass
class ProcessingConfig:
    """ì²˜ë¦¬ ì„¤ì • í´ë˜ìŠ¤"""
    # OCR ì„¤ì •
    dpi: int = 300
    language: str = "auto"
    confidence_threshold: int = 50
    oem: int = 3  # OCR Engine Mode
    psm: int = 6  # Page Segmentation Mode
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
    workers: int = 4
    max_memory_usage: int = 2048  # MB
    chunk_size: int = 10  # í•œ ë²ˆì— ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜
    
    # ìµœì í™” ì„¤ì •
    auto_dpi: bool = False
    auto_language: bool = False
    skip_blank_pages: bool = True
    
    # ì¶œë ¥ ì„¤ì •
    save_json: bool = False
    save_images: bool = False
    output_format: str = "pdf"  # pdf, txt, json
    
    # ë³µêµ¬ ë° ìºì‹œ ì„¤ì •
    resume: bool = True
    keep_checkpoints: bool = False
    cache_enabled: bool = True
    cache_ttl: int = 86400  # 24ì‹œê°„
    
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì„¤ì •
    preprocessing_enabled: bool = True
    binarization_threshold: int = 140
    noise_removal: bool = False
    deskew: bool = False
    enhance_contrast: bool = True
    enhance_sharpness: bool = False
    
    # í’ˆì§ˆ ì œì–´
    min_word_length: int = 2
    max_word_length: int = 50
    filter_numeric_only: bool = False
    
    # ë¡œê¹… ì„¤ì •
    log_level: str = "INFO"
    progress_bar: bool = True
    
    def validate(self) -> None:
        """ì„¤ì • ê²€ì¦"""
        if self.dpi < 72 or self.dpi > 600:
            raise ValueError("DPIëŠ” 72-600 ë²”ìœ„ì—¬ì•¼ í•©ë‹ˆë‹¤")
        if self.confidence_threshold < 0 or self.confidence_threshold > 100:
            raise ValueError("ì‹ ë¢°ë„ ì„ê³„ê°’ì€ 0-100 ë²”ìœ„ì—¬ì•¼ í•©ë‹ˆë‹¤")
        if self.workers < 1 or self.workers > 32:
            raise ValueError("ì›Œì»¤ ìˆ˜ëŠ” 1-32 ë²”ìœ„ì—¬ì•¼ í•©ë‹ˆë‹¤")
        if self.output_format not in ["pdf", "txt", "json"]:
            raise ValueError("ì¶œë ¥ í˜•ì‹ì€ pdf, txt, json ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤")


@dataclass
class WordData:
    """OCR ë‹¨ì–´ ë°ì´í„°"""
    text: str
    confidence: int
    bbox: Dict[str, int]
    page_number: int
    font_size: Optional[int] = None
    is_bold: bool = False
    is_italic: bool = False
    
    @property
    def area(self) -> int:
        """ë°”ìš´ë”© ë°•ìŠ¤ ë©´ì """
        return self.bbox["w"] * self.bbox["h"]
    
    @property
    def center(self) -> Tuple[int, int]:
        """ë°”ìš´ë”© ë°•ìŠ¤ ì¤‘ì‹¬ì """
        return (
            self.bbox["x"] + self.bbox["w"] // 2,
            self.bbox["y"] + self.bbox["h"] // 2
        )


@dataclass 
class PageResult:
    """í˜ì´ì§€ ì²˜ë¦¬ ê²°ê³¼"""
    page_number: int
    status: ProcessingStatus
    words: List[WordData] = field(default_factory=list)
    language: str = "eng"
    processing_time: float = 0.0
    error_message: Optional[str] = None
    
    # í†µê³„ ì •ë³´
    total_words: int = 0
    avg_confidence: float = 0.0
    is_blank: bool = False
    image_size: Optional[Tuple[int, int]] = None
    
    def __post_init__(self):
        """í›„ì²˜ë¦¬ - í†µê³„ ê³„ì‚°"""
        self.total_words = len(self.words)
        if self.words:
            self.avg_confidence = sum(w.confidence for w in self.words) / len(self.words)
            self.is_blank = self.total_words == 0
    
    @property
    def text(self) -> str:
        """í˜ì´ì§€ì˜ ëª¨ë“  í…ìŠ¤íŠ¸"""
        return " ".join(word.text for word in self.words)
    
    @property
    def high_confidence_words(self) -> List[WordData]:
        """ë†’ì€ ì‹ ë¢°ë„ ë‹¨ì–´ë“¤"""
        return [w for w in self.words if w.confidence >= 80]


@dataclass
class DocumentResult:
    """ë¬¸ì„œ ì²˜ë¦¬ ê²°ê³¼"""
    file_path: Path
    total_pages: int
    processed_pages: int
    status: ProcessingStatus
    pages: List[PageResult] = field(default_factory=list)
    processing_time: float = 0.0
    config: Optional[ProcessingConfig] = None
    
    # í†µê³„ ì •ë³´
    total_words: int = 0
    avg_confidence: float = 0.0
    detected_languages: List[str] = field(default_factory=list)
    blank_pages: List[int] = field(default_factory=list)
    error_pages: List[int] = field(default_factory=list)
    
    def __post_init__(self):
        """í›„ì²˜ë¦¬ - í†µê³„ ê³„ì‚°"""
        if self.pages:
            # ì „ì²´ ë‹¨ì–´ ìˆ˜
            self.total_words = sum(page.total_words for page in self.pages)
            
            # í‰ê·  ì‹ ë¢°ë„
            all_confidences = [
                word.confidence 
                for page in self.pages 
                for word in page.words
            ]
            if all_confidences:
                self.avg_confidence = sum(all_confidences) / len(all_confidences)
            
            # ì–¸ì–´ ê°ì§€
            self.detected_languages = list(set(page.language for page in self.pages))
            
            # ë¹ˆ í˜ì´ì§€ì™€ ì˜¤ë¥˜ í˜ì´ì§€
            self.blank_pages = [p.page_number for p in self.pages if p.is_blank]
            self.error_pages = [p.page_number for p in self.pages if p.status == ProcessingStatus.FAILED]
    
    @property
    def success_rate(self) -> float:
        """ì„±ê³µë¥ """
        if self.total_pages == 0:
            return 0.0
        return (self.processed_pages - len(self.error_pages)) / self.total_pages
    
    @property
    def full_text(self) -> str:
        """ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸"""
        return "\n\n".join(page.text for page in self.pages if not page.is_blank)
    
    def get_page_by_number(self, page_num: int) -> Optional[PageResult]:
        """í˜ì´ì§€ ë²ˆí˜¸ë¡œ í˜ì´ì§€ ê²°ê³¼ ì¡°íšŒ"""
        for page in self.pages:
            if page.page_number == page_num:
                return page
        return None
    
    def export_summary(self) -> Dict:
        """ê²°ê³¼ ìš”ì•½ ë‚´ë³´ë‚´ê¸°"""
        return {
            "file": str(self.file_path),
            "total_pages": self.total_pages,
            "processed_pages": self.processed_pages,
            "success_rate": self.success_rate,
            "total_words": self.total_words,
            "avg_confidence": round(self.avg_confidence, 2),
            "processing_time": round(self.processing_time, 2),
            "detected_languages": self.detected_languages,
            "blank_pages": len(self.blank_pages),
            "error_pages": len(self.error_pages),
            "status": self.status.value
        }


class CacheProtocol(Protocol):
    """ìºì‹œ ì¸í„°í˜ì´ìŠ¤"""
    def get(self, key: str) -> Optional[Any]: ...
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None: ...
    def exists(self, key: str) -> bool: ...
    def clear(self) -> None: ...


class MemoryCache:
    """ë©”ëª¨ë¦¬ ê¸°ë°˜ ìºì‹œ êµ¬í˜„"""
    
    def __init__(self):
        self._cache: Dict[str, Tuple[Any, float]] = {}
    
    def get(self, key: str) -> Optional[Any]:
        if key in self._cache:
            value, expires = self._cache[key]
            if expires == 0 or time.time() < expires:
                return value
            else:
                del self._cache[key]
        return None
    
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:
        expires = time.time() + ttl if ttl else 0
        self._cache[key] = (value, expires)
    
    def exists(self, key: str) -> bool:
        return self.get(key) is not None
    
    def clear(self) -> None:
        self._cache.clear()


class ImagePreprocessor:
    """ê³ ê¸‰ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
    
    def preprocess(self, image: Image.Image) -> Image.Image:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ìˆ˜í–‰"""
        if not self.config.preprocessing_enabled:
            return image
        
        processed_image = image.copy()
        
        try:
            # 1. ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if processed_image.mode != 'L':
                processed_image = ImageOps.grayscale(processed_image)
            
            # 2. ë…¸ì´ì¦ˆ ì œê±°
            if self.config.noise_removal:
                processed_image = self._remove_noise(processed_image)
            
            # 3. ê¸°ìš¸ì–´ì§ ë³´ì •
            if self.config.deskew:
                processed_image = self._deskew_image(processed_image)
            
            # 4. ëŒ€ë¹„ í–¥ìƒ
            if self.config.enhance_contrast:
                processed_image = self._enhance_contrast(processed_image)
            
            # 5. ì„ ëª…ë„ í–¥ìƒ
            if self.config.enhance_sharpness:
                processed_image = self._enhance_sharpness(processed_image)
            
            # 6. ì´ì§„í™”
            processed_image = self._binarize(processed_image)
            
            return processed_image
            
        except Exception as e:
            self.logger.warning(f"ì „ì²˜ë¦¬ ì‹¤íŒ¨, ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©: {e}")
            return image
    
    def _remove_noise(self, image: Image.Image) -> Image.Image:
        """ë…¸ì´ì¦ˆ ì œê±°"""
        if not CV2_AVAILABLE:
            # PIL ê¸°ë°˜ ë…¸ì´ì¦ˆ ì œê±°
            return image.filter(ImageFilter.MedianFilter(size=3))
        
        # OpenCV ê¸°ë°˜ ê³ ê¸‰ ë…¸ì´ì¦ˆ ì œê±°
        img_array = np.array(image)
        denoised = cv2.fastNlMeansDenoising(img_array, h=10, templateWindowSize=7, searchWindowSize=21)
        return Image.fromarray(denoised)
    
    def _deskew_image(self, image: Image.Image) -> Image.Image:
        """ê¸°ìš¸ì–´ì§ ë³´ì •"""
        if not CV2_AVAILABLE:
            return image
        
        try:
            img_array = np.array(image)
            
            # ê°€ì¥ìë¦¬ ê²€ì¶œ
            edges = cv2.Canny(img_array, 50, 150, apertureSize=3)
            
            # í—ˆí”„ ë³€í™˜ìœ¼ë¡œ ì§ì„  ê²€ì¶œ
            lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=100)
            
            if lines is not None:
                # ê°ë„ ê³„ì‚°
                angles = []
                for line in lines[:10]:  # ìƒìœ„ 10ê°œ ì§ì„ ë§Œ ì‚¬ìš©
                    rho, theta = line[0]
                    angle = theta * 180 / np.pi - 90
                    if -45 < angle < 45:
                        angles.append(angle)
                
                if angles:
                    # ì¤‘ê°„ê°’ ì‚¬ìš© (ì´ìƒì¹˜ ì œê±°)
                    median_angle = np.median(angles)
                    
                    # íšŒì „
                    if abs(median_angle) > 0.5:  # 0.5ë„ ì´ìƒ ê¸°ìš¸ì–´ì§„ ê²½ìš°ë§Œ ë³´ì •
                        rotated = image.rotate(-median_angle, expand=True, fillcolor='white')
                        return rotated
            
            return image
            
        except Exception as e:
            self.logger.warning(f"ê¸°ìš¸ì–´ì§ ë³´ì • ì‹¤íŒ¨: {e}")
            return image
    
    def _enhance_contrast(self, image: Image.Image) -> Image.Image:
        """ëŒ€ë¹„ í–¥ìƒ"""
        # íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™”
        image_array = np.array(image)
        
        if CV2_AVAILABLE:
            # CLAHE (Contrast Limited Adaptive Histogram Equalization)
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            enhanced = clahe.apply(image_array)
            return Image.fromarray(enhanced)
        else:
            # PIL ê¸°ë°˜ ëŒ€ë¹„ ì¡°ì •
            return ImageOps.autocontrast(image, cutoff=2)
    
    def _enhance_sharpness(self, image: Image.Image) -> Image.Image:
        """ì„ ëª…ë„ í–¥ìƒ"""
        enhancer = ImageEnhance.Sharpness(image)
        return enhancer.enhance(1.5)  # 50% ì„ ëª…ë„ ì¦ê°€
    
    def _binarize(self, image: Image.Image) -> Image.Image:
        """ì´ì§„í™”"""
        if CV2_AVAILABLE:
            # OpenCV ê¸°ë°˜ ì ì‘ì  ì„ê³„ê°’
            img_array = np.array(image)
            binary = cv2.adaptiveThreshold(
                img_array, 
                255, 
                cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                cv2.THRESH_BINARY, 
                11, 
                2
            )
            return Image.fromarray(binary)
        else:
            # PIL ê¸°ë°˜ ë‹¨ìˆœ ì„ê³„ê°’
            return image.point(
                lambda x: 0 if x < self.config.binarization_threshold else 255,
                "1"
            )
    
    def analyze_image_quality(self, image: Image.Image) -> Dict[str, float]:
        """ì´ë¯¸ì§€ í’ˆì§ˆ ë¶„ì„"""
        if not CV2_AVAILABLE:
            return {"quality_score": 0.5}
        
        try:
            img_array = np.array(image)
            
            # ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚° (ì„ ëª…ë„ ì¸¡ì •)
            laplacian_var = cv2.Laplacian(img_array, cv2.CV_64F).var()
            
            # ëŒ€ë¹„ ì¸¡ì •
            contrast = img_array.std()
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-1)
            sharpness_score = min(laplacian_var / 1000, 1.0)
            contrast_score = min(contrast / 128, 1.0)
            
            quality_score = (sharpness_score + contrast_score) / 2
            
            return {
                "quality_score": quality_score,
                "sharpness": sharpness_score,
                "contrast": contrast_score,
                "laplacian_variance": laplacian_var
            }
            
        except Exception as e:
            self.logger.warning(f"í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"quality_score": 0.5}


class LanguageDetector:
    """ê³ ê¸‰ ì–¸ì–´ ê°ì§€ í´ë˜ìŠ¤"""
    
    # ì–¸ì–´ ì½”ë“œ ë§¤í•‘
    LANG_MAPPING = {
        'ko': 'kor+eng',
        'kr': 'kor+eng', 
        'en': 'eng',
        'ja': 'jpn+eng',
        'zh-cn': 'chi_sim+eng',
        'zh': 'chi_sim+eng',
        'zh-tw': 'chi_tra+eng',
        'de': 'deu+eng',
        'fr': 'fra+eng',
        'es': 'spa+eng',
        'it': 'ita+eng',
        'pt': 'por+eng',
        'ru': 'rus+eng',
        'ar': 'ara+eng'
    }
    
    # ì–¸ì–´ë³„ íŠ¹ì„± íŒ¨í„´
    LANGUAGE_PATTERNS = {
        'kor': r'[ê°€-í£]',
        'jpn': r'[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠä¸€-é¾¯]',
        'chi_sim': r'[ä¸€-é¾¯]',
        'ara': r'[\u0600-\u06FF]',
        'rus': r'[Ğ°-ÑÑ‘]'
    }
    
    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
    
    def detect_language(self, image: Image.Image, text_sample: Optional[str] = None, fallback: str = "eng") -> str:
        """ê³ ê¸‰ ì–¸ì–´ ê°ì§€"""
        
        # 1. ì„¤ì •ì—ì„œ ìë™ ê°ì§€ê°€ ë¹„í™œì„±í™”ëœ ê²½ìš°
        if not self.config.auto_language and self.config.language != "auto":
            return self.config.language
        
        # 2. ì œê³µëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ ë¶„ì„
        if text_sample:
            detected = self._analyze_text_patterns(text_sample)
            if detected:
                return detected
        
        # 3. ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ ë¶„ì„
        if LANG_DETECT_AVAILABLE:
            detected = self._detect_from_image(image, fallback)
            if detected:
                return detected
        
        # 4. OCR ê¸°ë°˜ ì–¸ì–´ ê°ì§€
        detected = self._ocr_based_detection(image, fallback)
        if detected:
            return detected
        
        return fallback
    
    def _analyze_text_patterns(self, text: str) -> Optional[str]:
        """í…ìŠ¤íŠ¸ íŒ¨í„´ ë¶„ì„ìœ¼ë¡œ ì–¸ì–´ ê°ì§€"""
        import re
        
        text = text.strip()
        if len(text) < 10:  # ë„ˆë¬´ ì§§ìœ¼ë©´ ì‹ ë¢°í•˜ê¸° ì–´ë ¤ì›€
            return None
        
        # ê° ì–¸ì–´ íŒ¨í„´ ë§¤ì¹­
        scores = {}
        for lang, pattern in self.LANGUAGE_PATTERNS.items():
            matches = len(re.findall(pattern, text))
            if matches > 0:
                scores[lang] = matches / len(text)
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì–¸ì–´ ì„ íƒ
        if scores:
            best_lang = max(scores, key=scores.get)
            if scores[best_lang] > 0.1:  # ìµœì†Œ 10% ì´ìƒ ë§¤ì¹­
                return self.LANG_MAPPING.get(best_lang, best_lang)
        
        return None
    
    def _detect_from_image(self, image: Image.Image, fallback: str) -> Optional[str]:
        """ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ langdetect ì‚¬ìš©"""
        try:
            # ë¹ ë¥¸ OCRë¡œ ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            temp_text = pytesseract.image_to_string(
                image, 
                lang=fallback,
                config='--psm 6 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzê°€-í£ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠä¸€-é¾¯'
            ).strip()
            
            if len(temp_text) < 20:
                return None
            
            # langdetectë¡œ ì–¸ì–´ ê°ì§€
            detected = lang_detect(temp_text)
            mapped_lang = self.LANG_MAPPING.get(detected, detected)
            
            self.logger.info(f"ì–¸ì–´ ê°ì§€ë¨: {detected} -> {mapped_lang}")
            return mapped_lang
            
        except Exception as e:
            self.logger.warning(f"langdetect ê¸°ë°˜ ì–¸ì–´ ê°ì§€ ì‹¤íŒ¨: {e}")
            return None
    
    def _ocr_based_detection(self, image: Image.Image, fallback: str) -> Optional[str]:
        """ì—¬ëŸ¬ ì–¸ì–´ë¡œ OCR ìˆ˜í–‰í•˜ì—¬ ìµœì  ì–¸ì–´ ì°¾ê¸°"""
        try:
            candidate_languages = ['eng', 'kor+eng', 'jpn+eng', 'chi_sim+eng']
            best_lang = fallback
            best_score = 0
            
            for lang in candidate_languages:
                try:
                    # OCR ìˆ˜í–‰
                    data = pytesseract.image_to_data(
                        image,
                        lang=lang,
                        output_type=pytesseract.Output.DICT,
                        config='--psm 6'
                    )
                    
                    # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
                    confidences = [
                        int(conf) for conf in data.get('conf', [])
                        if conf != '-1' and int(conf) > 0
                    ]
                    
                    if confidences:
                        avg_confidence = sum(confidences) / len(confidences)
                        if avg_confidence > best_score:
                            best_score = avg_confidence
                            best_lang = lang
                
                except Exception:
                    continue
            
            return best_lang if best_score > self.config.confidence_threshold else fallback
            
        except Exception as e:
            self.logger.warning(f"OCR ê¸°ë°˜ ì–¸ì–´ ê°ì§€ ì‹¤íŒ¨: {e}")
            return fallback
    
    @classmethod
    def get_supported_languages(cls) -> List[str]:
        """ì§€ì›ë˜ëŠ” ì–¸ì–´ ëª©ë¡ ë°˜í™˜"""
        return list(cls.LANG_MAPPING.keys())
    
    def validate_language(self, language: str) -> str:
        """ì–¸ì–´ ì½”ë“œ ê²€ì¦ ë° ì •ê·œí™”"""
        if language in self.LANG_MAPPING:
            return self.LANG_MAPPING[language]
        elif language in self.LANG_MAPPING.values():
            return language
        else:
            self.logger.warning(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì–¸ì–´: {language}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return "eng"


class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.metrics = {
            "start_time": 0,
            "end_time": 0,
            "pages_processed": 0,
            "total_words_extracted": 0,
            "avg_confidence": 0.0,
            "memory_peak": 0,
            "cpu_usage": [],
            "processing_times": [],
            "error_count": 0,
            "cache_hits": 0,
            "cache_misses": 0
        }
        self._timers = {}
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.metrics["start_time"] = time.time()
        try:
            import psutil
            self.process = psutil.Process()
            self.initial_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        except ImportError:
            self.process = None
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ"""
        self.metrics["end_time"] = time.time()
    
    @contextmanager
    def time_operation(self, operation_name: str):
        """ì‘ì—… ì‹œê°„ ì¸¡ì •"""
        start = time.time()
        try:
            yield
        finally:
            duration = time.time() - start
            if operation_name not in self._timers:
                self._timers[operation_name] = []
            self._timers[operation_name].append(duration)
    
    def record_page_processed(self, page_result: PageResult):
        """í˜ì´ì§€ ì²˜ë¦¬ ê¸°ë¡"""
        self.metrics["pages_processed"] += 1
        self.metrics["total_words_extracted"] += page_result.total_words
        self.metrics["processing_times"].append(page_result.processing_time)
        
        if page_result.status == ProcessingStatus.FAILED:
            self.metrics["error_count"] += 1
        
        # í‰ê·  ì‹ ë¢°ë„ ê°±ì‹ 
        if page_result.words:
            all_confidences = [w.confidence for w in page_result.words]
            self.metrics["avg_confidence"] = (
                self.metrics["avg_confidence"] * (self.metrics["pages_processed"] - 1) +
                sum(all_confidences) / len(all_confidences)
            ) / self.metrics["pages_processed"]
    
    def record_cache_event(self, hit: bool):
        """ìºì‹œ ì´ë²¤íŠ¸ ê¸°ë¡"""
        if hit:
            self.metrics["cache_hits"] += 1
        else:
            self.metrics["cache_misses"] += 1
    
    def update_system_metrics(self):
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        if self.process:
            try:
                memory_mb = self.process.memory_info().rss / 1024 / 1024
                self.metrics["memory_peak"] = max(self.metrics["memory_peak"], memory_mb)
                
                cpu_percent = self.process.cpu_percent()
                self.metrics["cpu_usage"].append(cpu_percent)
            except Exception:
                pass
    
    def get_summary(self) -> Dict:
        """ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜"""
        total_time = self.metrics["end_time"] - self.metrics["start_time"]
        
        return {
            "ì´_ì²˜ë¦¬_ì‹œê°„": f"{total_time:.2f}ì´ˆ",
            "ì²˜ë¦¬ëœ_í˜ì´ì§€": self.metrics["pages_processed"],
            "í‰ê· _í˜ì´ì§€_ì²˜ë¦¬_ì‹œê°„": f"{sum(self.metrics['processing_times']) / max(len(self.metrics['processing_times']), 1):.2f}ì´ˆ",
            "í˜ì´ì§€_ì²˜ë¦¬_ì†ë„": f"{self.metrics['pages_processed'] / max(total_time, 1):.2f}í˜ì´ì§€/ì´ˆ",
            "ì¶”ì¶œëœ_ë‹¨ì–´_ìˆ˜": self.metrics["total_words_extracted"],
            "í‰ê· _ì‹ ë¢°ë„": f"{self.metrics['avg_confidence']:.2f}%",
            "ë©”ëª¨ë¦¬_í”¼í¬": f"{self.metrics['memory_peak']:.1f}MB",
            "í‰ê· _CPU_ì‚¬ìš©ë¥ ": f"{sum(self.metrics['cpu_usage']) / max(len(self.metrics['cpu_usage']), 1):.1f}%" if self.metrics['cpu_usage'] else "N/A",
            "ì˜¤ë¥˜_í˜ì´ì§€": self.metrics["error_count"],
            "ìºì‹œ_ì ì¤‘ë¥ ": f"{self.metrics['cache_hits'] / max(self.metrics['cache_hits'] + self.metrics['cache_misses'], 1) * 100:.1f}%",
            "ì‘ì—…ë³„_ì‹œê°„": {name: f"{sum(times):.2f}ì´ˆ" for name, times in self._timers.items()}
        }


class DPIOptimizer:
    """DPI ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.candidate_dpis = [200, 300, 400, 500]
    
    def find_optimal_dpi(self, pdf_path: Path, sample_pages: int = 2) -> int:
        """ìµœì  DPI ì°¾ê¸°"""
        if not self.config.auto_dpi:
            return self.config.dpi
        
        best_dpi = self.candidate_dpis[0]
        best_score = -1
        total_pages = self._get_pdf_page_count(pdf_path)
        
        with tqdm(
            total=min(sample_pages, total_pages) * len(self.candidate_dpis),
            desc="ğŸ” DPI ìµœì í™”",
            unit="test"
        ) as pbar:
            for page_num in range(1, min(sample_pages + 1, total_pages + 1)):
                for dpi in self.candidate_dpis:
                    try:
                        score = self._evaluate_dpi(pdf_path, page_num, dpi)
                        if score > best_score:
                            best_score = score
                            best_dpi = dpi
                    except Exception:
                        pass
                    finally:
                        pbar.update(1)
        
        return best_dpi
    
    def _evaluate_dpi(self, pdf_path: Path, page_num: int, dpi: int) -> int:
        """íŠ¹ì • DPIë¡œ í˜ì´ì§€ í‰ê°€"""
        images = convert_from_path(
            str(pdf_path), 
            dpi=dpi, 
            first_page=page_num, 
            last_page=page_num
        )
        
        if not images:
            return 0
        
        preprocessor = ImagePreprocessor(self.config)
        processed_image = preprocessor.preprocess(images[0])
        
        data = pytesseract.image_to_data(
            processed_image, 
            output_type=pytesseract.Output.DICT
        )
        
        # ì‹ ë¢°ë„ ë†’ì€ ë‹¨ì–´ ìˆ˜ ë°˜í™˜
        return sum(
            1 for i, conf in enumerate(data.get('conf', []))
            if data.get('text', [None] * len(data.get('conf', [])))[i] and 
            int(conf) >= self.config.confidence_threshold
        )
    
    @staticmethod
    def _get_pdf_page_count(pdf_path: Path) -> int:
        """PDF í˜ì´ì§€ ìˆ˜ ë°˜í™˜"""
        return len(PdfReader(str(pdf_path)).pages)


class CheckpointManager:
    """ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, base_dir: Path):
        self.base_dir = base_dir
        self.checkpoint_dir = base_dir / ".checkpoints"
    
    def create_checkpoint_dir(self, document_id: str) -> Path:
        """ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±"""
        checkpoint_path = self.checkpoint_dir / document_id
        checkpoint_path.mkdir(parents=True, exist_ok=True)
        return checkpoint_path
    
    def save_manifest(self, checkpoint_path: Path, manifest: Dict) -> None:
        """ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥"""
        manifest_file = checkpoint_path / "manifest.json"
        manifest_file.write_text(
            json.dumps(manifest, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )
    
    def load_manifest(self, checkpoint_path: Path) -> Optional[Dict]:
        """ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë¡œë“œ"""
        manifest_file = checkpoint_path / "manifest.json"
        if manifest_file.exists():
            return json.loads(manifest_file.read_text(encoding="utf-8"))
        return None
    
    def save_page_result(self, checkpoint_path: Path, page_result: PageResult, pdf_bytes: bytes) -> None:
        """í˜ì´ì§€ ê²°ê³¼ ì €ì¥"""
        # PDF ì €ì¥
        pdf_file = checkpoint_path / f"page_{page_result.page_number:05d}.pdf"
        pdf_file.write_bytes(pdf_bytes)
        
        # JSON ì €ì¥ (ì„¤ì •ì— ë”°ë¼)
        json_file = checkpoint_path / f"page_{page_result.page_number:05d}.json"
        page_data = {
            "page": page_result.page_number,
            "status": page_result.status.value,
            "language": page_result.language,
            "processing_time": page_result.processing_time,
            "words": [
                {
                    "text": word.text,
                    "confidence": word.confidence,
                    "bbox": word.bbox
                }
                for word in page_result.words
            ]
        }
        json_file.write_text(
            json.dumps(page_data, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )
    
    def cleanup_checkpoints(self, checkpoint_path: Path) -> None:
        """ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬"""
        for file in checkpoint_path.glob("*"):
            try:
                file.unlink()
            except Exception:
                pass
        try:
            checkpoint_path.rmdir()
        except Exception:
            pass


class OCRProcessor:
    """ê³ ê¸‰ OCR ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config: ProcessingConfig, cache: Optional[CacheProtocol] = None):
        self.config = config
        self.cache = cache or MemoryCache()
        self.preprocessor = ImagePreprocessor(config)
        self.language_detector = LanguageDetector(config)
        self.performance_monitor = PerformanceMonitor()
        self.logger = logging.getLogger(__name__)
        
        # Tesseract ì„¤ì • ê²€ì¦
        self._validate_tesseract_config()
    
    def _validate_tesseract_config(self):
        """Tesseract ì„¤ì • ê²€ì¦"""
        try:
            # Tesseract ë²„ì „ í™•ì¸
            version = pytesseract.get_tesseract_version()
            self.logger.info(f"Tesseract ë²„ì „: {version}")
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ì–¸ì–´ í™•ì¸
            languages = pytesseract.get_languages()
            self.logger.info(f"ì§€ì› ì–¸ì–´: {languages}")
            
        except Exception as e:
            self.logger.warning(f"Tesseract ì„¤ì • ê²€ì¦ ì‹¤íŒ¨: {e}")
    
    def process_image(self, image: Image.Image, page_number: int, language: str = None) -> PageResult:
        """ë‹¨ì¼ ì´ë¯¸ì§€ OCR ì²˜ë¦¬"""
        start_time = time.time()
        
        try:
            with self.performance_monitor.time_operation("image_preprocessing"):
                # ì´ë¯¸ì§€ í’ˆì§ˆ ë¶„ì„
                quality_metrics = self.preprocessor.analyze_image_quality(image)
                
                # ìºì‹œ í‚¤ ìƒì„±
                cache_key = self._generate_cache_key(image, language or self.config.language)
                
                if self.config.cache_enabled and self.cache.exists(cache_key):
                    cached_result = self.cache.get(cache_key)
                    if cached_result:
                        cached_result.page_number = page_number
                        self.performance_monitor.record_cache_event(True)
                        return cached_result
                
                self.performance_monitor.record_cache_event(False)
                
                # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                processed_image = self.preprocessor.preprocess(image)
            
            with self.performance_monitor.time_operation("language_detection"):
                # ì–¸ì–´ ê°ì§€
                detected_language = language or self.language_detector.detect_language(processed_image)
            
            with self.performance_monitor.time_operation("ocr_extraction"):
                # OCR ìˆ˜í–‰
                words = self._extract_words(processed_image, detected_language, page_number)
                
                # ë¹ˆ í˜ì´ì§€ ê±´ë„ˆë›°ê¸°
                if self.config.skip_blank_pages and len(words) == 0:
                    result = PageResult(
                        page_number=page_number,
                        status=ProcessingStatus.COMPLETED,
                        words=[],
                        language=detected_language,
                        processing_time=time.time() - start_time,
                        is_blank=True,
                        image_size=image.size
                    )
                else:
                    # í›„ì²˜ë¦¬
                    words = self._post_process_words(words)
                    
                    result = PageResult(
                        page_number=page_number,
                        status=ProcessingStatus.COMPLETED,
                        words=words,
                        language=detected_language,
                        processing_time=time.time() - start_time,
                        image_size=image.size
                    )
            
            # ìºì‹œ ì €ì¥
            if self.config.cache_enabled:
                self.cache.set(cache_key, result, self.config.cache_ttl)
            
            return result
            
        except Exception as e:
            self.logger.error(f"í˜ì´ì§€ {page_number} OCR ì‹¤íŒ¨: {e}")
            return PageResult(
                page_number=page_number,
                status=ProcessingStatus.FAILED,
                error_message=str(e),
                processing_time=time.time() - start_time,
                image_size=image.size if image else None
            )
    
    def _extract_words(self, image: Image.Image, language: str, page_number: int) -> List[WordData]:
        """ì´ë¯¸ì§€ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ"""
        
        # Tesseract ì„¤ì • êµ¬ì„±
        tesseract_config = self._build_tesseract_config()
        
        try:
            data = pytesseract.image_to_data(
                image, 
                lang=language, 
                output_type=pytesseract.Output.DICT,
                config=tesseract_config
            )
        except Exception as e:
            self.logger.error(f"Tesseract OCR ì‹¤íŒ¨: {e}")
            return []
        
        words = []
        text_data = data.get('text', [])
        conf_data = data.get('conf', [])
        
        for i in range(len(text_data)):
            text = (text_data[i] or "").strip()
            
            try:
                confidence = int(float(conf_data[i]))
            except (ValueError, TypeError, IndexError):
                confidence = -1
            
            # í•„í„°ë§ ì¡°ê±´
            if not self._should_include_word(text, confidence):
                continue
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´
            bbox = {
                "x": int(data.get('left', [0] * (i + 1))[i]),
                "y": int(data.get('top', [0] * (i + 1))[i]),
                "w": int(data.get('width', [0] * (i + 1))[i]),
                "h": int(data.get('height', [0] * (i + 1))[i]),
            }
            
            words.append(WordData(
                text=text,
                confidence=confidence,
                bbox=bbox,
                page_number=page_number,
                font_size=self._estimate_font_size(bbox)
            ))
        
        return words
    
    def _build_tesseract_config(self) -> str:
        """Tesseract ì„¤ì • ë¬¸ìì—´ êµ¬ì„±"""
        config_parts = []
        
        # OEM (OCR Engine Mode) ì„¤ì •
        config_parts.append(f"--oem {self.config.oem}")
        
        # PSM (Page Segmentation Mode) ì„¤ì •
        config_parts.append(f"--psm {self.config.psm}")
        
        # ì‹ ë¢°ë„ ì„¤ì •
        if self.config.confidence_threshold > 0:
            config_parts.append(f"-c tessedit_min_confidence={self.config.confidence_threshold}")
        
        return " ".join(config_parts)
    
    def _should_include_word(self, text: str, confidence: int) -> bool:
        """ë‹¨ì–´ í¬í•¨ ì—¬ë¶€ ê²°ì •"""
        if not text:
            return False
        
        # ì‹ ë¢°ë„ í•„í„°
        if confidence < self.config.confidence_threshold:
            return False
        
        # ê¸¸ì´ í•„í„°
        if len(text) < self.config.min_word_length or len(text) > self.config.max_word_length:
            return False
        
        # ìˆ«ìë§Œ í¬í•¨ëœ í…ìŠ¤íŠ¸ í•„í„° (ì˜µì…˜)
        if self.config.filter_numeric_only and text.isdigit():
            return False
        
        return True
    
    def _post_process_words(self, words: List[WordData]) -> List[WordData]:
        """ë‹¨ì–´ í›„ì²˜ë¦¬"""
        if not words:
            return words
        
        # ì¤‘ë³µ ì œê±° (ê°™ì€ ìœ„ì¹˜ì˜ ìœ ì‚¬í•œ ë‹¨ì–´)
        filtered_words = []
        for word in words:
            is_duplicate = False
            for existing_word in filtered_words:
                if self._are_words_duplicate(word, existing_word):
                    # ë” ë†’ì€ ì‹ ë¢°ë„ì˜ ë‹¨ì–´ ìœ ì§€
                    if word.confidence > existing_word.confidence:
                        filtered_words.remove(existing_word)
                        break
                    else:
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                filtered_words.append(word)
        
        return filtered_words
    
    def _are_words_duplicate(self, word1: WordData, word2: WordData) -> bool:
        """ë‘ ë‹¨ì–´ê°€ ì¤‘ë³µì¸ì§€ í™•ì¸"""
        # ìœ„ì¹˜ê°€ ë¹„ìŠ·í•˜ê³  í…ìŠ¤íŠ¸ê°€ ìœ ì‚¬í•œ ê²½ìš°
        bbox1, bbox2 = word1.bbox, word2.bbox
        
        # ë°”ìš´ë”© ë°•ìŠ¤ ê²¹ì¹¨ í™•ì¸
        overlap_x = max(0, min(bbox1["x"] + bbox1["w"], bbox2["x"] + bbox2["w"]) - max(bbox1["x"], bbox2["x"]))
        overlap_y = max(0, min(bbox1["y"] + bbox1["h"], bbox2["y"] + bbox2["h"]) - max(bbox1["y"], bbox2["y"]))
        
        overlap_area = overlap_x * overlap_y
        min_area = min(bbox1["w"] * bbox1["h"], bbox2["w"] * bbox2["h"])
        
        if min_area == 0:
            return False
        
        overlap_ratio = overlap_area / min_area
        
        # 50% ì´ìƒ ê²¹ì¹˜ê³  í…ìŠ¤íŠ¸ê°€ ìœ ì‚¬í•œ ê²½ìš°
        if overlap_ratio > 0.5:
            # ë¬¸ìì—´ ìœ ì‚¬ë„ í™•ì¸ (ê°„ë‹¨í•œ ë°©ë²•)
            shorter = min(len(word1.text), len(word2.text))
            if shorter == 0:
                return False
            
            common_chars = sum(c1 == c2 for c1, c2 in zip(word1.text, word2.text))
            similarity = common_chars / shorter
            
            return similarity > 0.8
        
        return False
    
    def _estimate_font_size(self, bbox: Dict[str, int]) -> Optional[int]:
        """ë°”ìš´ë”© ë°•ìŠ¤ì—ì„œ í°íŠ¸ í¬ê¸° ì¶”ì •"""
        return bbox["h"]  # ë†’ì´ë¥¼ í°íŠ¸ í¬ê¸°ë¡œ ê·¼ì‚¬
    
    def _generate_cache_key(self, image: Image.Image, language: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # ì´ë¯¸ì§€ í•´ì‹œ + ì„¤ì • í•´ì‹œ
        image_bytes = BytesIO()
        image.save(image_bytes, format='PNG')
        image_hash = hashlib.md5(image_bytes.getvalue()).hexdigest()
        
        config_str = f"{language}_{self.config.confidence_threshold}_{self.config.binarization_threshold}_{self.config.oem}_{self.config.psm}"
        config_hash = hashlib.md5(config_str.encode()).hexdigest()
        
        return f"{image_hash}_{config_hash}"
    
    def create_searchable_pdf(self, image: Image.Image, language: str) -> bytes:
        """ê²€ìƒ‰ ê°€ëŠ¥í•œ PDF ìƒì„±"""
        try:
            processed_image = self.preprocessor.preprocess(image)
            config = self._build_tesseract_config()
            
            return pytesseract.image_to_pdf_or_hocr(
                processed_image, 
                lang=language, 
                extension="pdf",
                config=config
            )
        except Exception as e:
            self.logger.error(f"ê²€ìƒ‰ ê°€ëŠ¥í•œ PDF ìƒì„± ì‹¤íŒ¨: {e}")
            # ë¹ˆ PDF ë°˜í™˜
            writer = PdfWriter()
            bio = BytesIO()
            writer.write(bio)
            return bio.getvalue()
    
    def extract_text_only(self, image: Image.Image, language: str) -> str:
        """í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ë¹ ë¥¸ ë°©ë²•)"""
        try:
            processed_image = self.preprocessor.preprocess(image)
            config = self._build_tesseract_config()
            
            text = pytesseract.image_to_string(
                processed_image,
                lang=language,
                config=config
            )
            
            return text.strip()
            
        except Exception as e:
            self.logger.error(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""


class PDFProcessor:
    """PDF ì²˜ë¦¬ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, config: ProcessingConfig, cache: Optional[CacheProtocol] = None):
        self.config = config
        self.cache = cache or MemoryCache()
        self.ocr_processor = OCRProcessor(config, cache)
        self.dpi_optimizer = DPIOptimizer(config)
        self.checkpoint_manager = CheckpointManager(Path("./output"))
        self.logger = self._setup_logger()
    
    async def process_document(self, file_path: Path, output_dir: Path) -> DocumentResult:
        """ë¬¸ì„œ ì²˜ë¦¬ (ë¹„ë™ê¸°)"""
        start_time = time.time()
        
        try:
            # ì„¤ì • ê²€ì¦
            self.config.validate()
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
            monitor = PerformanceMonitor()
            monitor.start_monitoring()
            
            # ë¬¸ì„œ ID ìƒì„±
            document_id = file_path.stem
            
            # ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
            checkpoint_path = self.checkpoint_manager.create_checkpoint_dir(document_id)
            
            with monitor.time_operation("dpi_optimization"):
                # ìµœì  DPI ê²°ì •
                optimal_dpi = self.dpi_optimizer.find_optimal_dpi(file_path)
                self.logger.info(f"ğŸ“Š ìµœì  DPI ì„ íƒ: {optimal_dpi}")
            
            # PDF ì²˜ë¦¬
            result = await self._process_pdf_async(
                file_path, 
                output_dir, 
                checkpoint_path, 
                optimal_dpi,
                monitor
            )
            
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
            monitor.stop_monitoring()
            
            result.processing_time = time.time() - start_time
            result.config = self.config
            
            # ì„±ëŠ¥ ìš”ì•½ ë¡œê¹…
            performance_summary = monitor.get_summary()
            self.logger.info("ğŸ“ˆ ì„±ëŠ¥ ìš”ì•½:")
            for key, value in performance_summary.items():
                self.logger.info(f"  {key}: {value}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return DocumentResult(
                file_path=file_path,
                total_pages=0,
                processed_pages=0,
                status=ProcessingStatus.FAILED,
                processing_time=time.time() - start_time
            )
    
    async def _process_pdf_async(self, 
                                 pdf_path: Path, 
                                 output_dir: Path, 
                                 checkpoint_path: Path, 
                                 dpi: int,
                                 monitor: PerformanceMonitor) -> DocumentResult:
        """ë¹„ë™ê¸° PDF ì²˜ë¦¬"""
        
        total_pages = self.dpi_optimizer._get_pdf_page_count(pdf_path)
        self.logger.info(f"ğŸ“– ì´ í˜ì´ì§€ ìˆ˜: {total_pages}")
        
        # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë¡œë“œ/ìƒì„±
        manifest = self.checkpoint_manager.load_manifest(checkpoint_path)
        if not manifest:
            manifest = {
                "file": pdf_path.name,
                "dpi": dpi,
                "total_pages": total_pages,
                "completed_pages": [],
                "config": self.config.__dict__,
                "created_at": time.time()
            }
        
        completed_pages = set(manifest.get("completed_pages", []))
        
        # ì²˜ë¦¬í•  í˜ì´ì§€ ê²°ì •
        pending_pages = [
            i for i in range(1, total_pages + 1) 
            if i not in completed_pages
        ]
        
        if not pending_pages:
            self.logger.info("âœ… ëª¨ë“  í˜ì´ì§€ê°€ ì´ë¯¸ ì²˜ë¦¬ë¨")
            return await self._finalize_document(pdf_path, checkpoint_path, output_dir, manifest)
        
        self.logger.info(f"ğŸ”„ ì²˜ë¦¬í•  í˜ì´ì§€: {len(pending_pages)}ê°œ")
        
        # ì²­í¬ ë‹¨ìœ„ë¡œ í˜ì´ì§€ ì²˜ë¦¬
        page_results = []
        chunk_size = min(self.config.chunk_size, len(pending_pages))
        
        for i in range(0, len(pending_pages), chunk_size):
            chunk = pending_pages[i:i + chunk_size]
            self.logger.info(f"ğŸ“„ ì²­í¬ ì²˜ë¦¬ ì¤‘: {i+1}-{min(i+chunk_size, len(pending_pages))} / {len(pending_pages)}")
            
            # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
            monitor.update_system_metrics()
            
            chunk_results = await self._process_pages_parallel(
                pdf_path, 
                chunk,
                dpi, 
                checkpoint_path,
                monitor
            )
            
            page_results.extend(chunk_results)
            
            # ì¤‘ê°„ ì €ì¥
            for result in chunk_results:
                if result.status == ProcessingStatus.COMPLETED:
                    completed_pages.add(result.page_number)
                    monitor.record_page_processed(result)
            
            manifest["completed_pages"] = sorted(list(completed_pages))
            manifest["last_updated"] = time.time()
            self.checkpoint_manager.save_manifest(checkpoint_path, manifest)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if i % (chunk_size * 3) == 0:  # 3ê°œ ì²­í¬ë§ˆë‹¤
                import gc
                gc.collect()
        
        # ìµœì¢… ë¬¸ì„œ ìƒì„±
        return await self._finalize_document(pdf_path, checkpoint_path, output_dir, manifest)
    
    async def _process_pages_parallel(self, 
                                      pdf_path: Path, 
                                      page_numbers: List[int], 
                                      dpi: int, 
                                      checkpoint_path: Path,
                                      monitor: PerformanceMonitor) -> List[PageResult]:
        """í˜ì´ì§€ ë³‘ë ¬ ì²˜ë¦¬ (ê°œì„ ë¨)"""
        
        loop = asyncio.get_event_loop()
        results = []
        
        with ProcessPoolExecutor(max_workers=self.config.workers) as executor:
            # ì‘ì—… ìƒì„±
            tasks = []
            for page_num in page_numbers:
                task = loop.run_in_executor(
                    executor,
                    self._process_single_page,
                    pdf_path,
                    page_num,
                    dpi
                )
                tasks.append((task, page_num))
            
            # ì§„í–‰ë¥  í‘œì‹œ (ì˜µì…˜)
            if self.config.progress_bar:
                progress_bar = tqdm(
                    total=len(tasks), 
                    desc=f"ğŸ“„ {pdf_path.name}", 
                    unit="í˜ì´ì§€",
                    ncols=100,
                    bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
                )
            else:
                progress_bar = None
            
            # ì™„ë£Œëœ ì‘ì—… ìˆ˜ì§‘
            for completed_task, page_num in asyncio.as_completed([(task, num) for task, num in tasks]):
                try:
                    result = await completed_task
                    results.append(result)
                    
                    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                    if result.status == ProcessingStatus.COMPLETED:
                        with monitor.time_operation("checkpoint_save"):
                            # PDF ë°”ì´íŠ¸ ìƒì„±
                            images = convert_from_path(
                                str(pdf_path), 
                                dpi=dpi, 
                                first_page=result.page_number, 
                                last_page=result.page_number
                            )
                            if images:
                                pdf_bytes = self.ocr_processor.create_searchable_pdf(
                                    images[0], 
                                    result.language
                                )
                                self.checkpoint_manager.save_page_result(
                                    checkpoint_path, 
                                    result, 
                                    pdf_bytes
                                )
                    
                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    if progress_bar:
                        progress_bar.set_postfix({
                            'ìƒíƒœ': result.status.value[:4],
                            'ì‹ ë¢°ë„': f"{result.avg_confidence:.0f}%",
                            'ë‹¨ì–´': result.total_words
                        })
                        progress_bar.update(1)
                        
                except Exception as e:
                    self.logger.error(f"í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    results.append(PageResult(
                        page_number=page_num,
                        status=ProcessingStatus.FAILED,
                        error_message=str(e)
                    ))
                    
                    if progress_bar:
                        progress_bar.update(1)
            
            if progress_bar:
                progress_bar.close()
        
        return results
    
    def _process_single_page(self, pdf_path: Path, page_number: int, dpi: int) -> PageResult:
        """ë‹¨ì¼ í˜ì´ì§€ ì²˜ë¦¬ (ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ìš©)"""
        try:
            # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            images = convert_from_path(
                str(pdf_path), 
                dpi=dpi, 
                first_page=page_number, 
                last_page=page_number
            )
            
            if not images:
                return PageResult(
                    page_number=page_number,
                    status=ProcessingStatus.FAILED,
                    error_message="ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨"
                )
            
            # OCR ì²˜ë¦¬
            return self.ocr_processor.process_image(
                images[0], 
                page_number, 
                self.config.language
            )
            
        except Exception as e:
            return PageResult(
                page_number=page_number,
                status=ProcessingStatus.FAILED,
                error_message=str(e)
            )
    
    async def _finalize_document(self, 
                                 pdf_path: Path, 
                                 checkpoint_path: Path, 
                                 output_dir: Path, 
                                 manifest: Dict) -> DocumentResult:
        """ë¬¸ì„œ ìµœì¢…í™”"""
        
        # PDF ë³‘í•©
        output_pdf = output_dir / f"{pdf_path.stem}_searchable.pdf"
        await self._merge_pdfs(checkpoint_path, output_pdf)
        
        # JSON ë³‘í•© (ì˜µì…˜)
        if self.config.save_json:
            await self._merge_json_files(checkpoint_path, output_pdf.with_suffix(".json"))
        
        # ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬
        if not self.config.keep_checkpoints:
            self.checkpoint_manager.cleanup_checkpoints(checkpoint_path)
        
        # ê²°ê³¼ ìƒì„±
        total_pages = manifest["total_pages"]
        completed_pages = len(manifest.get("completed_pages", []))
        
        return DocumentResult(
            file_path=pdf_path,
            total_pages=total_pages,
            processed_pages=completed_pages,
            status=ProcessingStatus.COMPLETED if completed_pages == total_pages else ProcessingStatus.FAILED
        )
    
    async def _merge_pdfs(self, checkpoint_path: Path, output_path: Path) -> None:
        """PDF íŒŒì¼ë“¤ ë³‘í•©"""
        writer = PdfWriter()
        pdf_files = sorted(checkpoint_path.glob("page_*.pdf"))
        
        for pdf_file in pdf_files:
            reader = PdfReader(str(pdf_file))
            for page in reader.pages:
                writer.add_page(page)
        
        # ë¹„ë™ê¸°ë¡œ ì“°ê¸°
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            None, 
            lambda: output_path.write_bytes(self._write_pdf_to_bytes(writer))
        )
    
    async def _merge_json_files(self, checkpoint_path: Path, output_path: Path) -> None:
        """JSON íŒŒì¼ë“¤ ë³‘í•©"""
        merged_data = {
            "type": "pdf",
            "pages": []
        }
        
        json_files = sorted(checkpoint_path.glob("page_*.json"))
        for json_file in json_files:
            page_data = json.loads(json_file.read_text(encoding="utf-8"))
            merged_data["pages"].append(page_data)
        
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            None,
            lambda: output_path.write_text(
                json.dumps(merged_data, ensure_ascii=False, indent=2),
                encoding="utf-8"
            )
        )
    
    def _write_pdf_to_bytes(self, writer: PdfWriter) -> bytes:
        """PDF Writerë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜"""
        bio = BytesIO()
        writer.write(bio)
        return bio.getvalue()
    
    def _setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger(__name__)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s | %(levelname)s | %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger


# íŒ©í† ë¦¬ íŒ¨í„´
class ProcessorFactory:
    """í”„ë¡œì„¸ì„œ íŒ©í† ë¦¬"""
    
    @staticmethod
    def create_processor(config: ProcessingConfig, cache: Optional[CacheProtocol] = None) -> PDFProcessor:
        """í”„ë¡œì„¸ì„œ ìƒì„±"""
        return PDFProcessor(config, cache)
    
    @staticmethod
    def create_config(**kwargs) -> ProcessingConfig:
        """ì„¤ì • ìƒì„±"""
        return ProcessingConfig(**kwargs)


# ë°°ì¹˜ ì²˜ë¦¬ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
async def process_documents_batch(
    file_paths: List[Path], 
    output_dir: Path, 
    config: ProcessingConfig,
    max_concurrent: int = 3
) -> List[DocumentResult]:
    """ì—¬ëŸ¬ ë¬¸ì„œ ë°°ì¹˜ ì²˜ë¦¬ (ë™ì‹œ ì‹¤í–‰ ì œí•œ)"""
    
    processor = ProcessorFactory.create_processor(config)
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def process_single(file_path: Path) -> DocumentResult:
        async with semaphore:
            return await processor.process_document(file_path, output_dir)
    
    # ëª¨ë“  ì‘ì—… ì‹œì‘
    tasks = [process_single(file_path) for file_path in file_paths]
    
    results = []
    total_files = len(file_paths)
    
    # ì§„í–‰ë¥  í‘œì‹œ
    if config.progress_bar:
        progress = tqdm(
            total=total_files, 
            desc="ğŸ“ ë°°ì¹˜ ì²˜ë¦¬", 
            unit="íŒŒì¼",
            ncols=100
        )
    else:
        progress = None
    
    for completed_task in asyncio.as_completed(tasks):
        result = await completed_task
        results.append(result)
        
        if progress:
            status_emoji = "âœ…" if result.status == ProcessingStatus.COMPLETED else "âŒ"
            progress.set_postfix({
                'íŒŒì¼': result.file_path.name[:20],
                'ìƒíƒœ': status_emoji,
                'í˜ì´ì§€': f"{result.processed_pages}/{result.total_pages}"
            })
            progress.update(1)
    
    if progress:
        progress.close()
    
    return results


def create_processing_report(results: List[DocumentResult], output_path: Optional[Path] = None) -> Dict:
    """ì²˜ë¦¬ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±"""
    
    total_files = len(results)
    successful_files = sum(1 for r in results if r.status == ProcessingStatus.COMPLETED)
    failed_files = total_files - successful_files
    
    total_pages = sum(r.total_pages for r in results)
    processed_pages = sum(r.processed_pages for r in results)
    total_words = sum(r.total_words for r in results)
    
    avg_confidence = sum(r.avg_confidence for r in results if r.avg_confidence > 0) / max(successful_files, 1)
    total_processing_time = sum(r.processing_time for r in results)
    
    # ì–¸ì–´ë³„ í†µê³„
    language_stats = {}
    for result in results:
        for lang in result.detected_languages:
            language_stats[lang] = language_stats.get(lang, 0) + 1
    
    report = {
        "ì²˜ë¦¬_ìš”ì•½": {
            "ì´_íŒŒì¼_ìˆ˜": total_files,
            "ì„±ê³µí•œ_íŒŒì¼": successful_files,
            "ì‹¤íŒ¨í•œ_íŒŒì¼": failed_files,
            "ì„±ê³µë¥ ": f"{successful_files / max(total_files, 1) * 100:.1f}%"
        },
        "í˜ì´ì§€_í†µê³„": {
            "ì´_í˜ì´ì§€": total_pages,
            "ì²˜ë¦¬ëœ_í˜ì´ì§€": processed_pages,
            "í˜ì´ì§€_ì²˜ë¦¬ìœ¨": f"{processed_pages / max(total_pages, 1) * 100:.1f}%"
        },
        "OCR_ì„±ëŠ¥": {
            "ì¶”ì¶œëœ_ë‹¨ì–´": total_words,
            "í‰ê· _ì‹ ë¢°ë„": f"{avg_confidence:.2f}%",
            "í‰ê· _í˜ì´ì§€ë‹¹_ë‹¨ì–´": f"{total_words / max(processed_pages, 1):.1f}ê°œ"
        },
        "ì‹œê°„_ì„±ëŠ¥": {
            "ì´_ì²˜ë¦¬_ì‹œê°„": f"{total_processing_time:.2f}ì´ˆ",
            "í‰ê· _íŒŒì¼_ì²˜ë¦¬_ì‹œê°„": f"{total_processing_time / max(total_files, 1):.2f}ì´ˆ",
            "í˜ì´ì§€_ì²˜ë¦¬_ì†ë„": f"{processed_pages / max(total_processing_time, 1):.2f}í˜ì´ì§€/ì´ˆ"
        },
        "ì–¸ì–´_í†µê³„": language_stats,
        "ì‹¤íŒ¨í•œ_íŒŒì¼ë“¤": [
            {
                "íŒŒì¼": str(r.file_path),
                "ì˜¤ë¥˜_í˜ì´ì§€": len(r.error_pages),
                "ì²˜ë¦¬_ì‹œê°„": f"{r.processing_time:.2f}ì´ˆ"
            }
            for r in results if r.status == ProcessingStatus.FAILED
        ]
    }
    
    # ë³´ê³ ì„œ ì €ì¥ (ì˜µì…˜)
    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
    
    return report


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def example_usage():
    """ì‚¬ìš© ì˜ˆì‹œ"""
    
    # 1. ê¸°ë³¸ ì„¤ì •
    config = ProcessorFactory.create_config(
        dpi=300,
        language="auto",
        auto_dpi=True,
        auto_language=True,
        workers=4,
        save_json=True,
        cache_enabled=True,
        preprocessing_enabled=True,
        noise_removal=True,
        enhance_contrast=True,
        skip_blank_pages=True,
        progress_bar=True
    )
    
    # 2. ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
    print("=== ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ì˜ˆì‹œ ===")
    processor = ProcessorFactory.create_processor(config)
    
    # result = await processor.process_document(
    #     Path("sample.pdf"),
    #     Path("./output")
    # )
    # print(f"ì²˜ë¦¬ ê²°ê³¼: {result.status.value}")
    # print(f"ì¶”ì¶œëœ ë‹¨ì–´: {result.total_words}ê°œ")
    # print(f"í‰ê·  ì‹ ë¢°ë„: {result.avg_confidence:.2f}%")
    
    # 3. ë°°ì¹˜ ì²˜ë¦¬
    print("\n=== ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì‹œ ===")
    file_paths = [
        # Path("document1.pdf"),
        # Path("document2.pdf"),
        # Path("document3.pdf")
    ]
    
    if file_paths:
        results = await process_documents_batch(
            file_paths,
            Path("./output"),
            config,
            max_concurrent=2
        )
        
        # 4. ë³´ê³ ì„œ ìƒì„±
        report = create_processing_report(results, Path("./output/processing_report.json"))
        
        print("ğŸ“Š ì²˜ë¦¬ ë³´ê³ ì„œ:")
        for section, data in report.items():
            if isinstance(data, dict):
                print(f"\n{section}:")
                for key, value in data.items():
                    print(f"  {key}: {value}")
            else:
                print(f"{section}: {data}")


if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    async def main():
        try:
            await example_usage()
        except KeyboardInterrupt:
            print("\nâ¹ï¸  ì²˜ë¦¬ ì¤‘ë‹¨ë¨")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
    
    # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
    try:
        asyncio.run(main())
    except RuntimeError:
        # Jupyter ë“±ì—ì„œ ì‹¤í–‰ì‹œ
        import nest_asyncio
        nest_asyncio.apply()
        asyncio.get_event_loop().run_until_complete(main())